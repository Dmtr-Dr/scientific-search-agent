"""
AI Agent –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangGraph –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ–∏—Å–∫–∞
"""

import os
# FIX: –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã OpenMP conflict —Å FAISS
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

from typing import TypedDict, List, Dict, Any, Optional
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import json
from dotenv import load_dotenv
from pydantic import BaseModel, Field, ValidationError

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


# ============================================================================
# –£–¢–ò–õ–ò–¢–´: –ù–∞–¥—ë–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
# ============================================================================

def safe_json_parse(response_text: str, fallback: Any = None) -> Any:
    """
    –ù–∞–¥—ë–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫
    """
    try:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –≤ —Ç–µ–∫—Å—Ç–µ
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx == -1 or end_idx == -1:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –º–∞—Å—Å–∏–≤
            start_idx = response_text.find('[')
            end_idx = response_text.rfind(']')
        
        if start_idx == -1 or end_idx == -1:
            return fallback
        
        json_str = response_text[start_idx:end_idx + 1]
        
        # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ –≤—Ä—É—á–Ω—É—é
            try:
                # –£–±–∏—Ä–∞–µ–º trailing commas
                json_str = json_str.replace(',}', '}').replace(',]', ']')
                # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                lines = json_str.split('\n')
                cleaned = [line.split('//')[0] for line in lines]
                json_str = '\n'.join(cleaned)
                return json.loads(json_str)
            except Exception:
                return fallback
    
    except Exception as e:
        return fallback


# ============================================================================
# PYDANTIC –ú–û–î–ï–õ–ò –î–õ–Ø –í–ê–õ–ò–î–ê–¶–ò–ò
# ============================================================================

class TopicCardModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è TopicCard"""
    must: List[str] = Field(default_factory=list)
    should: List[str] = Field(default_factory=list)
    must_not: List[str] = Field(default_factory=list)
    synonyms: List[str] = Field(default_factory=list)
    expanded_queries: List[str] = Field(default_factory=list)
    fields_of_study: List[str] = Field(default_factory=list)


class StructuredSummaryModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
    problem: str = ""
    methods: List[str] = Field(default_factory=list)
    datasets: List[str] = Field(default_factory=list)
    metrics: List[str] = Field(default_factory=list)
    key_findings: str = ""
    limitations: str = ""
    future_work: str = ""
    contributions: str = ""  # –í–∫–ª–∞–¥ —Ä–∞–±–æ—Ç—ã
    related_work_summary: str = ""  # –ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç
    experimental_setup: str = ""  # –î–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    reproducibility_info: str = ""  # –ù–∞–ª–∏—á–∏–µ –∫–æ–¥–∞/–¥–∞–Ω–Ω—ã—Ö
    discussion: str = ""  # –û–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    conclusion: str = ""  # –í—ã–≤–æ–¥—ã


class ResearchGapModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è research gap"""
    gap: str
    type: str  # methodological|data|metric|reproducibility|contradiction|temporal|scalability|cross_domain
    severity: str  # high|medium|low
    evidence: List[str] = Field(default_factory=list)
    reasoning: str = ""  # –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø–æ—á–µ–º—É —ç—Ç–æ gap
    potential_impact: str = ""  # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    related_methods: List[str] = Field(default_factory=list)  # –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    feasibility: str = ""  # –û—Ü–µ–Ω–∫–∞ –≤—ã–ø–æ–ª–Ω–∏–º–æ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è


class ResearchIdeaModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∏–¥–µ–∏"""
    hypothesis: str
    experiment_plan: Dict[str, Any] = Field(default_factory=dict)
    expected_outcome: str = ""
    risks: List[str] = Field(default_factory=list)
    related_gap: str = ""


# –û–ø—Ä–µ–¥–µ–ª—è–µ–º State –∞–≥–µ–Ω—Ç–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
class AgentState(TypedDict):
    """
    –°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - –ø–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    """
    # === –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ===
    query: str  # –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    time_window: int  # –û–∫–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ (–ª–µ—Ç –Ω–∞–∑–∞–¥)
    max_papers: int  # –ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    # === –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ ===
    selected_databases: List[str]  # –í—ã–±—Ä–∞–Ω–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞
    refined_query: str  # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    
    # === TopicCard (QueryBuilder) ===
    topic_card: Dict[str, Any]  # must[], should[], must_not[], synonyms[]
    query_strings: List[str]  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    
    # === –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ ===
    search_results: Dict[str, List[Dict[str, Any]]]  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    
    # === SeedResults (Retriever) ===
    seed_results: List[Dict[str, Any]]  # –°—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ API
    
    # === CorpusIndex (Deduper/Normalizer) ===
    corpus_index: List[Dict[str, Any]]  # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    
    # === Ranked papers (Ranker) ===
    ranked_papers: List[Dict[str, Any]]  # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
    
    # === PDF Reader ===
    pdf_texts: Dict[str, str]  # –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã PDF: {paper_id: full_text}
    
    # === Citation graph (Snowballer) ===
    citation_graph: Dict[str, Any]  # nodes[], edges[], centrality{}
    
    # === LitMatrix (Summarizer) ===
    lit_matrix: List[Dict[str, Any]]  # –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
    
    # === Research gaps (GapMiner) ===
    gap_list: List[Dict[str, Any]]  # –°–ø–∏—Å–æ–∫ –ª–∞–∫—É–Ω
    
    # === Ideas (Ideator) ===
    idea_bank: List[Dict[str, Any]]  # –ì–∏–ø–æ—Ç–µ–∑—ã –∏ –∏–¥–µ–∏
    
    # === –°–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è ===
    final_response: str  # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
    messages: List[Any]  # –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π LLM
    next_step: str  # –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –≤ pipeline
    budget: Dict[str, int]  # –ë—é–¥–∂–µ—Ç API –≤—ã–∑–æ–≤–æ–≤
    retry_count: int  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø–æ–∏—Å–∫–∞
    search_quality_score: float  # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    replanning_history: List[str]  # –ò—Å—Ç–æ—Ä–∏—è –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–π


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
def get_llm():
    """–°–æ–∑–¥–∞—ë—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä LLM"""
    from config import LLM_CONFIG
    return ChatOpenAI(
        model=LLM_CONFIG["model"],
        temperature=LLM_CONFIG["temperature"],
        max_retries=LLM_CONFIG["max_retries"],
        api_key=os.getenv("OPENAI_API_KEY")
    )


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Embeddings
def get_embeddings():
    """–°–æ–∑–¥–∞—ë—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è embeddings"""
    from langchain_openai import OpenAIEmbeddings
    return OpenAIEmbeddings(
        model="text-embedding-3-small",  # –î–µ—à—ë–≤–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
        api_key=os.getenv("OPENAI_API_KEY")
    )


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 1: –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
# ============================================================================

def analyze_query(state: AgentState) -> AgentState:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
    1. –ö–∞–∫–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    2. –ö–∞–∫ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    print("\nüîç –§—É–Ω–∫—Ü–∏—è 1: –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞...")
    
    query = state["query"]
    llm = get_llm()
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞
    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. 
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
    1. –ö–∞–∫–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (arxiv, pubmed, –∏–ª–∏ –æ–±–∞)
    2. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
    
    –í—ã–±–∏—Ä–∞–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
    - arxiv: —Ñ–∏–∑–∏–∫–∞, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, computer science, –±–∏–æ–ª–æ–≥–∏—è (–ø—Ä–µ–ø—Ä–∏–Ω—Ç—ã)
    - pubmed: –º–µ–¥–∏—Ü–∏–Ω–∞, –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω–∞, –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    
    –û—Ç–≤–µ—Ç—å –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
    {
        "databases": ["arxiv", "pubmed"],
        "refined_query": "–ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å"
    }
    """
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}")
    ]
    
    response = llm.invoke(messages)
    
    # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    parsed = safe_json_parse(response.content, {})
    
    state["selected_databases"] = parsed.get("databases", ["arxiv"]) if isinstance(parsed, dict) else ["arxiv"]
    state["refined_query"] = parsed.get("refined_query", query) if isinstance(parsed, dict) else query
    
    if not isinstance(parsed, dict):
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    state["messages"] = messages + [response]
    state["next_step"] = "search"
    
    print(f"   üìä –í—ã–±—Ä–∞–Ω–Ω—ã–µ –ë–î: {state['selected_databases']}")
    print(f"   üìù –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {state['refined_query']}")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 2: –ü–æ–∏—Å–∫ –≤ ArXiv
# ============================================================================

def search_arxiv(query: str, max_results: int = 5) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö ArXiv
    """
    print(f"\nüìö –§—É–Ω–∫—Ü–∏—è 2a: –ü–æ–∏—Å–∫ –≤ ArXiv –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'...")
    
    import arxiv
    
    try:
        # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance
        )
        
        results = []
        for paper in search.results():
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ArXiv ID –∏–∑ entry_id (—Ñ–æ—Ä–º–∞—Ç: http://arxiv.org/abs/1234.5678v1)
            arxiv_id = None
            if paper.entry_id:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
                import re
                match = re.search(r'/(\d{4}\.\d{4,5})(?:v\d+)?', paper.entry_id)
                if match:
                    arxiv_id = match.group(1)
            
            results.append({
                "title": paper.title,
                "authors": [author.name for author in paper.authors],
                "summary": paper.summary[:500] + "..." if len(paper.summary) > 500 else paper.summary,
                "url": paper.entry_id,
                "arxiv_id": arxiv_id,
                "published": paper.published.strftime("%Y-%m-%d"),
                "categories": paper.categories,
                "source": "arxiv"
            })
        
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—Ç–∞—Ç–µ–π –≤ ArXiv")
        return results
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ ArXiv: {e}")
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 3: –ü–æ–∏—Å–∫ –≤ PubMed
# ============================================================================

def search_pubmed(query: str, max_results: int = 5) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö PubMed —á–µ—Ä–µ–∑ Entrez API
    """
    print(f"\nüè• –§—É–Ω–∫—Ü–∏—è 2b: –ü–æ–∏—Å–∫ –≤ PubMed –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'...")
    
    from Bio import Entrez
    import xml.etree.ElementTree as ET
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º email –¥–ª—è Entrez (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ NCBI)
    Entrez.email = "dru4inin.dmitry@gmail.com"
    
    try:
        # –ü–æ–∏—Å–∫ ID —Å—Ç–∞—Ç–µ–π
        handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results)
        record = Entrez.read(handle)
        handle.close()
        
        id_list = record["IdList"]
        
        if not id_list:
            print("   ‚úì –°—Ç–∞—Ç—å–∏ –≤ PubMed –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ —Å—Ç–∞—Ç–µ–π
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="xml", retmode="xml")
        xml_data = handle.read()
        handle.close()
        
        # –ü–∞—Ä—Å–∏–º XML
        root = ET.fromstring(xml_data)
        
        results = []
        for article in root.findall(".//PubmedArticle"):
            try:
                title_elem = article.find(".//ArticleTitle")
                title = title_elem.text if title_elem is not None else "No title"
                
                abstract_elem = article.find(".//AbstractText")
                abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
                if abstract and len(abstract) > 500:
                    abstract = abstract[:500] + "..."
                
                # –ê–≤—Ç–æ—Ä—ã
                authors = []
                for author in article.findall(".//Author"):
                    lastname = author.find("LastName")
                    forename = author.find("ForeName")
                    if lastname is not None and forename is not None:
                        authors.append(f"{forename.text} {lastname.text}")
                
                # PMID
                pmid_elem = article.find(".//PMID")
                pmid = pmid_elem.text if pmid_elem is not None else ""
                
                # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = article.find(".//PubDate/Year")
                year = pub_date.text if pub_date is not None else "Unknown"
                
                results.append({
                    "title": title,
                    "authors": authors,
                    "summary": abstract,
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                    "published": year,
                    "source": "pubmed",
                    "pmid": pmid
                })
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
                continue
        
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—Ç–∞—Ç–µ–π –≤ PubMed")
        return results
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ PubMed: {e}")
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 4: –£–∑–µ–ª –≥—Ä–∞—Ñ–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
# ============================================================================

def perform_search(state: AgentState) -> AgentState:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    print("\nüîé –§—É–Ω–∫—Ü–∏—è 4: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    query = state["refined_query"]
    databases = state["selected_databases"]
    
    search_results = {}
    
    # –ü–æ–∏—Å–∫ –≤ –∫–∞–∂–¥–æ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    if "arxiv" in databases:
        search_results["arxiv"] = search_arxiv(query, max_results=5)
    
    if "pubmed" in databases:
        search_results["pubmed"] = search_pubmed(query, max_results=5)
    
    state["search_results"] = search_results
    state["next_step"] = "synthesize"
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    total_results = sum(len(results) for results in search_results.values())
    print(f"\n   üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {total_results}")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 5: –ü–æ–∏—Å–∫ –≤ OpenAlex
# ============================================================================

def search_openalex(query: str, max_results: int = 50, from_year: int = 2019) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ OpenAlex - –º–æ—â–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π
    """
    print(f"\nüåê –§—É–Ω–∫—Ü–∏—è 5: –ü–æ–∏—Å–∫ –≤ OpenAlex –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'...")
    
    import requests
    import time
    
    try:
        base_url = "https://api.openalex.org/works"
        
        params = {
            "search": query,
            "filter": f"from_publication_date:{from_year}-01-01",
            "per_page": min(max_results, 50),
            "mailto": "researcher@example.com"  # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å –¥–ª—è API
        }
        
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        results = []
        
        for work in data.get("results", []):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º DOI
            doi = work.get("doi", "").replace("https://doi.org/", "") if work.get("doi") else None
            
            # –ê–≤—Ç–æ—Ä—ã
            authors = []
            for authorship in work.get("authorships", [])[:10]:
                author = authorship.get("author", {})
                if author.get("display_name"):
                    authors.append(author["display_name"])
            
            # –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            year = work.get("publication_year", "Unknown")
            
            # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            citations_total = work.get("cited_by_count", 0)
            
            # Abstract (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å)
            abstract = ""
            abstract_inv = work.get("abstract_inverted_index", {})
            if abstract_inv:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
                word_positions = []
                for word, positions in abstract_inv.items():
                    for pos in positions:
                        word_positions.append((pos, word))
                word_positions.sort()
                abstract = " ".join([word for _, word in word_positions])
                if len(abstract) > 500:
                    abstract = abstract[:500] + "..."
            
            results.append({
                "title": work.get("title", "No title"),
                "authors": authors,
                "summary": abstract or "No abstract available",
                "url": work.get("id", ""),
                "doi": doi,
                "published": str(year),
                "citations_total": citations_total,
                "source": "openalex",
                "venue": work.get("primary_location", {}).get("source", {}).get("display_name", "Unknown"),
                "type": work.get("type", "article")
            })
        
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—Ç–∞—Ç–µ–π –≤ OpenAlex")
        time.sleep(0.1)  # –í–µ–∂–ª–∏–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        return results
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ OpenAlex: {e}")
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 6: –ü–æ–∏—Å–∫ –≤ Semantic Scholar
# ============================================================================

def search_semantic_scholar(query: str, max_results: int = 50, from_year: int = 2019) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ Semantic Scholar - –æ—Ç–ª–∏—á–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤–ª–∏—è–Ω–∏—è
    """
    print(f"\nüéì –§—É–Ω–∫—Ü–∏—è 6: –ü–æ–∏—Å–∫ –≤ Semantic Scholar –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'...")
    
    import requests
    import time
    
    try:
        base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        
        params = {
            "query": query,
            "limit": min(max_results, 100),
            "fields": "title,authors,abstract,year,citationCount,url,externalIds,venue,publicationTypes,influentialCitationCount",
            "year": f"{from_year}-"
        }
        
        headers = {
            "Accept": "application/json"
        }
        
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        results = []
        
        for paper in data.get("data", []):
            # –ê–≤—Ç–æ—Ä—ã
            authors = [author.get("name", "") for author in paper.get("authors", [])]
            
            # DOI –∏ –¥—Ä—É–≥–∏–µ ID
            ext_ids = paper.get("externalIds", {})
            doi = ext_ids.get("DOI")
            arxiv_id = ext_ids.get("ArXiv")
            pmid = ext_ids.get("PubMed")
            
            # Abstract
            abstract = paper.get("abstract", "No abstract available")
            if abstract and len(abstract) > 500:
                abstract = abstract[:500] + "..."
            
            results.append({
                "title": paper.get("title", "No title"),
                "authors": authors,
                "summary": abstract,
                "url": paper.get("url", ""),
                "doi": doi,
                "arxiv_id": arxiv_id,
                "pmid": pmid,
                "published": str(paper.get("year", "Unknown")),
                "citations_total": paper.get("citationCount", 0),
                "influential_citations": paper.get("influentialCitationCount", 0),
                "source": "semantic_scholar",
                "venue": paper.get("venue", "Unknown"),
                "type": paper.get("publicationTypes", ["article"])[0] if paper.get("publicationTypes") else "article"
            })
        
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—Ç–∞—Ç–µ–π –≤ Semantic Scholar")
        time.sleep(0.1)  # –í–µ–∂–ª–∏–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        return results
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Semantic Scholar: {e}")
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 7: –ü–æ–∏—Å–∫ –≤ Crossref
# ============================================================================

def search_crossref(query: str, max_results: int = 50, from_year: int = 2019) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ Crossref - –∏—Å—Ç–æ—á–Ω–∏–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ DOI
    """
    print(f"\nüîó –§—É–Ω–∫—Ü–∏—è 7: –ü–æ–∏—Å–∫ –≤ Crossref –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'...")
    
    import requests
    import time
    
    try:
        base_url = "https://api.crossref.org/works"
        
        params = {
            "query": query,
            "filter": f"from-pub-date:{from_year}",
            "rows": min(max_results, 100),
            "mailto": "researcher@example.com"
        }
        
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        results = []
        
        for item in data.get("message", {}).get("items", []):
            # –ê–≤—Ç–æ—Ä—ã
            authors = []
            for author in item.get("author", [])[:10]:
                given = author.get("given", "")
                family = author.get("family", "")
                if given or family:
                    authors.append(f"{given} {family}".strip())
            
            # –ì–æ–¥
            pub_date = item.get("published-print") or item.get("published-online") or item.get("created")
            year = "Unknown"
            if pub_date and "date-parts" in pub_date:
                year = str(pub_date["date-parts"][0][0])
            
            # Abstract
            abstract = item.get("abstract", "No abstract available")
            if abstract and len(abstract) > 500:
                abstract = abstract[:500] + "..."
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞/–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
            venue = "Unknown"
            if item.get("container-title"):
                venue = item["container-title"][0] if isinstance(item["container-title"], list) else item["container-title"]
            
            results.append({
                "title": item.get("title", ["No title"])[0] if isinstance(item.get("title"), list) else item.get("title", "No title"),
                "authors": authors,
                "summary": abstract,
                "url": item.get("URL", ""),
                "doi": item.get("DOI"),
                "published": year,
                "citations_total": item.get("is-referenced-by-count", 0),
                "source": "crossref",
                "venue": venue,
                "type": item.get("type", "article")
            })
        
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—Ç–∞—Ç–µ–π –≤ Crossref")
        time.sleep(0.1)
        return results
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Crossref: {e}")
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 8: QueryBuilder - —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
# ============================================================================

def build_topic_card(state: AgentState) -> AgentState:
    """
    –°–æ–∑–¥–∞—ë—Ç TopicCard: –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, —Å–∏–Ω–æ–Ω–∏–º—ã,
    –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    """
    print("\nüî® –§—É–Ω–∫—Ü–∏—è 8: QueryBuilder - –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TopicCard...")
    
    query = state["query"]
    llm = get_llm()
    
    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞—É—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    
–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π TopicCard.

–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON —Ñ–æ—Ä–º–∞—Ç:
{
  "must": ["–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"],
  "should": ["–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã", "—Å–∏–Ω–æ–Ω–∏–º—ã", "—Å–º–µ–∂–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è"],
  "must_not": ["–∏—Å–∫–ª—é—á–µ–Ω–∏—è", "–Ω–∞–ø—Ä–∏–º–µ—Ä: review, survey, tutorial"],
  "synonyms": ["—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"],
  "expanded_queries": ["—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å 1", "–∑–∞–ø—Ä–æ—Å —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ 2", "–∑–∞–ø—Ä–æ—Å –ø–æ –ø–æ–¥—Ç–µ–º–µ 3"],
  "fields_of_study": ["–æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: machine learning, biology"]
}

–ü—Ä–∞–≤–∏–ª–∞:
1. –ò–∑–≤–ª–µ–∫–∞–π –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (5-10)
2. –î–æ–±–∞–≤–ª—è–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (ML = Machine Learning)
3. –°–æ–∑–¥–∞–π 3-5 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏
4. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∏—Å–∫–ª—é—á–∞–π reviews/surveys –µ—Å–ª–∏ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
"""
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"–¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {query}\n\n–í—Ä–µ–º—è: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {state.get('time_window', 5)} –ª–µ—Ç")
    ]
    
    response = llm.invoke(messages)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    try:
        from config import SEARCH_FILTERS
        exclude_reviews = SEARCH_FILTERS.get("exclude_reviews", False)
        novelty_level = SEARCH_FILTERS.get("novelty_level", None)
        domain = SEARCH_FILTERS.get("domain", None)
        
        filter_hints = []
        if exclude_reviews:
            filter_hints.append("–ò–°–ö–õ–Æ–ß–ò–¢–¨: review, survey, tutorial, overview —Å—Ç–∞—Ç—å–∏")
        if novelty_level == "high":
            filter_hints.append("–ü–†–ò–û–†–ò–¢–ï–¢: –æ—á–µ–Ω—å —Å–≤–µ–∂–∏–µ —Ä–∞–±–æ—Ç—ã (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 1-2 –≥–æ–¥–∞)")
        elif novelty_level == "low":
            filter_hints.append("–í–ö–õ–Æ–ß–ò–¢–¨: –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∏ —Å—Ç–∞—Ä—ã–µ —Ä–∞–±–æ—Ç—ã")
        if domain == "medicine":
            filter_hints.append("–§–û–ö–£–°: –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
        elif domain == "cs":
            filter_hints.append("–§–û–ö–£–°: computer science, AI, ML –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
        
        if filter_hints:
            filter_text = "\n".join(filter_hints)
            messages[1].content += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n{filter_text}"
    except:
        pass
    
    response = llm.invoke(messages)
    
    # –ü–∞—Ä—Å–∏–º TopicCard —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    parsed = safe_json_parse(response.content, {})
    
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ Pydantic
    try:
        topic_card_model = TopicCardModel(**parsed)
        topic_card = topic_card_model.model_dump()
    except ValidationError:
        # Fallback –Ω–∞ —Å—ã—Ä–æ–π dict
        topic_card = parsed if isinstance(parsed, dict) else {
            "must": [query],
            "should": [],
            "must_not": [],
            "synonyms": [],
            "expanded_queries": [query],
            "fields_of_study": []
        }
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∫ must_not
    if exclude_reviews:
        if "review" not in topic_card.get("must_not", []):
            topic_card.setdefault("must_not", []).extend(["review", "survey", "tutorial", "overview"])
    
    state["topic_card"] = topic_card
    state["query_strings"] = topic_card.get("expanded_queries", [query])
    
    print(f"   ‚úì TopicCard —Å–æ–∑–¥–∞–Ω:")
    print(f"     - –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {len(topic_card.get('must', []))}")
    print(f"     - –°–∏–Ω–æ–Ω–∏–º—ã: {len(topic_card.get('synonyms', []))}")
    print(f"     - –ò—Å–∫–ª—é—á–µ–Ω–∏—è: {len(topic_card.get('must_not', []))}")
    print(f"     - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(state['query_strings'])}")
    
    state["messages"] = messages + [response]
    state["next_step"] = "retrieve"
    
    return state


# ============================================================================
# –§–ê–ó–ê 4: –£–º–Ω—ã–π –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã (–ó–∞–¥–∞—á–∞ 4.1)
# ============================================================================

def analyze_query_and_select_sources(state: AgentState) -> AgentState:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ LLM –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—É—á–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
    """
    print("\nüß† –ê–Ω–∞–ª–∏–∑ —Ç–µ–º—ã –∏ –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    query = state["query"]
    llm = get_llm()
    
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –Ω–∞—É—á–Ω—É—é –æ–±–ª–∞—Å—Ç—å.

–ó–∞–ø—Ä–æ—Å: "{query}"

–í—ã–±–µ—Ä–∏ 1-2 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –æ–±–ª–∞—Å—Ç–∏ –∏–∑ —Å–ø–∏—Å–∫–∞:
- computer_science (AI, ML, NLP, computer vision, algorithms, software engineering)
- biomedicine (medicine, biology, genetics, pharmaceuticals, healthcare)
- physics (physics, astronomy, quantum mechanics, astrophysics)
- mathematics (pure math, applied math, statistics, computational math)
- chemistry (chemistry, materials science, nanotechnology, biochemistry)
- social_sciences (psychology, sociology, economics, political science, education)
- engineering (mechanical, electrical, civil engineering, robotics)
- general (–º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–µ –∏–ª–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã)

–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON:
{{
    "primary_field": "–Ω–∞–∑–≤–∞–Ω–∏–µ_–æ–±–ª–∞—Å—Ç–∏",
    "secondary_field": "–Ω–∞–∑–≤–∞–Ω–∏–µ_–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null",
    "confidence": 0.0-1.0,
    "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"
}}"""
    
    messages = [HumanMessage(content=prompt)]
    response = llm.invoke(messages)
    
    analysis = safe_json_parse(response.content, {
        "primary_field": "general",
        "secondary_field": None,
        "confidence": 0.5,
        "reasoning": "Could not determine field"
    })
    
    # –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–ª–∞—Å—Ç–∏
    primary_field = analysis.get("primary_field", "general")
    secondary_field = analysis.get("secondary_field")
    
    source_weights = select_sources_by_field(primary_field, secondary_field)
    
    state["selected_databases"] = source_weights
    state["field_analysis"] = analysis
    
    print(f"   üìö Detected field: {primary_field} (confidence: {analysis.get('confidence', 0.5):.2f})")
    if secondary_field:
        print(f"   üìö Secondary field: {secondary_field}")
    print(f"   üìä Selected sources: {source_weights}")
    print(f"   üí° Reasoning: {analysis.get('reasoning', '')[:100]}")
    
    return state


def select_sources_by_field(primary_field: str, secondary_field: str = None) -> Dict[str, float]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {–∏—Å—Ç–æ—á–Ω–∏–∫: –≤–µ—Å} –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—É—á–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
    
    –í–µ—Å–∞:
    - 1.0 = –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏
    - 0.5-0.9 = –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    - 0.0-0.3 = –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º (—Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è)
    """
    source_configs = {
        "computer_science": {
            "arxiv": 1.0,
            "semantic_scholar": 1.0,
            "openalex": 0.8,
            "crossref": 0.5,
            "pubmed": 0.0,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è CS
        },
        "biomedicine": {
            "pubmed": 1.0,
            "openalex": 0.9,
            "crossref": 0.7,
            "semantic_scholar": 0.5,
            "arxiv": 0.3,
        },
        "physics": {
            "arxiv": 1.0,
            "openalex": 0.9,
            "crossref": 0.7,
            "semantic_scholar": 0.6,
            "pubmed": 0.0,
        },
        "mathematics": {
            "arxiv": 1.0,
            "openalex": 0.9,
            "crossref": 0.7,
            "semantic_scholar": 0.6,
            "pubmed": 0.0,
        },
        "chemistry": {
            "openalex": 1.0,
            "crossref": 0.9,
            "pubmed": 0.6,
            "semantic_scholar": 0.5,
            "arxiv": 0.4,
        },
        "social_sciences": {
            "openalex": 1.0,
            "crossref": 0.9,
            "semantic_scholar": 0.7,
            "pubmed": 0.3,
            "arxiv": 0.0,
        },
        "engineering": {
            "openalex": 1.0,
            "semantic_scholar": 0.9,
            "crossref": 0.8,
            "arxiv": 0.6,
            "pubmed": 0.0,
        },
        "general": {
            "openalex": 0.8,
            "semantic_scholar": 0.8,
            "crossref": 0.8,
            "arxiv": 0.8,
            "pubmed": 0.8,
        }
    }
    
    weights = source_configs.get(primary_field, source_configs["general"])
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Ç–æ—Ä–∏—á–Ω–∞—è –æ–±–ª–∞—Å—Ç—å - —É—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ—Å–∞
    if secondary_field and secondary_field in source_configs:
        secondary_weights = source_configs[secondary_field]
        weights = {
            source: (weights.get(source, 0) * 0.7 + secondary_weights.get(source, 0) * 0.3)
            for source in set(list(weights.keys()) + list(secondary_weights.keys()))
        }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –≤–µ—Å–æ–º > 0.3 (—É–±–∏—Ä–∞–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ)
    filtered_weights = {
        source: weight
        for source, weight in weights.items()
        if weight > 0.3
    }
    
    return filtered_weights


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 9: Retriever - –º—É–ª—å—Ç–∏-–ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
# ============================================================================

def multi_source_retriever(state: AgentState) -> AgentState:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ TopicCard
    
    –£–õ–£–ß–®–ï–ù–ò–ï (–§–∞–∑–∞ 4): –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–º–Ω—ã–π –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã
    """
    print("\nüîç –§—É–Ω–∫—Ü–∏—è 9: Multi-source Retriever - –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º...")
    
    query_strings = state.get("query_strings", [state["query"]])
    time_window = state.get("time_window", 5)
    from_year = 2025 - time_window  # –¢–µ–∫—É—â–∏–π –≥–æ–¥ –º–∏–Ω—É—Å –æ–∫–Ω–æ
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π (–æ—Å–Ω–æ–≤–Ω–æ–π) –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
    main_query = query_strings[0] if query_strings else state["query"]
    
    # ============================================================================
    # –£–ú–ù–´–ô –í–´–ë–û–† –ò–°–¢–û–ß–ù–ò–ö–û–í (–§–∞–∑–∞ 4, –ó–∞–¥–∞—á–∞ 4.1)
    # ============================================================================
    source_weights = state.get("selected_databases", {})
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if not source_weights:
        print("   ‚ö†Ô∏è  Source selection not performed, using all sources")
        source_weights = {
            "openalex": 1.0,
            "semantic_scholar": 1.0,
            "crossref": 1.0,
            "arxiv": 1.0,
            "pubmed": 1.0
        }
    
    seed_results = []
    per_source = 30  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    
    print(f"   üìå –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å: '{main_query}'")
    print(f"   üìÖ –ü–µ—Ä–∏–æ–¥: —Å {from_year} –≥–æ–¥–∞")
    print(f"   üéØ –í—ã–±—Ä–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {list(source_weights.keys())}")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Å–æ–≤ (—Ç–æ–ª—å–∫–æ —Å –≤–µ—Å–æ–º > 0)
    sources = []
    
    if source_weights.get("openalex", 0) > 0:
        sources.append(("OpenAlex", lambda: search_openalex(main_query, max_results=per_source, from_year=from_year), source_weights["openalex"]))
    
    if source_weights.get("semantic_scholar", 0) > 0:
        sources.append(("Semantic Scholar", lambda: search_semantic_scholar(main_query, max_results=per_source, from_year=from_year), source_weights["semantic_scholar"]))
    
    if source_weights.get("crossref", 0) > 0:
        sources.append(("Crossref", lambda: search_crossref(main_query, max_results=per_source, from_year=from_year), source_weights["crossref"]))
    
    if source_weights.get("arxiv", 0) > 0:
        sources.append(("ArXiv", lambda: search_arxiv(main_query, max_results=per_source), source_weights["arxiv"]))
    
    if source_weights.get("pubmed", 0) > 0:
        sources.append(("PubMed", lambda: search_pubmed(main_query, max_results=per_source), source_weights["pubmed"]))
    
    for source_name, search_func, weight in sources:
        try:
            results = search_func()
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∫ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–µ (–¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è)
            for paper in results:
                paper["_source_weight"] = weight
            seed_results.extend(results)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ {source_name}: {e}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
    filtered_results = apply_search_filters(seed_results, state.get("topic_card", {}))
    
    state["seed_results"] = filtered_results
    state["next_step"] = "deduplicate"
    
    print(f"\n   üìä –°–æ–±—Ä–∞–Ω–æ —Å—ã—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(seed_results)}")
    print(f"   üîç –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_results)}")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –±—é–¥–∂–µ—Ç
    if "budget" not in state:
        state["budget"] = {}
    state["budget"]["api_calls"] = state["budget"].get("api_calls", 0) + len(sources)
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞
# ============================================================================

def apply_search_filters(results: List[Dict[str, Any]], topic_card: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞:
    - –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–æ–≤/surveys
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–æ–≤–∏–∑–Ω–µ
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–æ–º–µ–Ω—É
    """
    try:
        from config import SEARCH_FILTERS
        exclude_reviews = SEARCH_FILTERS.get("exclude_reviews", False)
        novelty_level = SEARCH_FILTERS.get("novelty_level", None)
        domain = SEARCH_FILTERS.get("domain", None)
    except:
        exclude_reviews = False
        novelty_level = None
        domain = None
    
    filtered = []
    must_not_terms = topic_card.get("must_not", [])
    
    for paper in results:
        title = paper.get("title", "").lower()
        summary = (paper.get("summary") or "").lower() if paper.get("summary") else ""
        text = f"{title} {summary}"
        
        # –§–∏–ª—å—Ç—Ä: –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–æ–≤
        if exclude_reviews or must_not_terms:
            exclude_terms = ["review", "survey", "tutorial", "overview", "overview of"]
            exclude_terms.extend([term.lower() for term in must_not_terms])
            
            is_review = any(term in text for term in exclude_terms)
            if is_review:
                continue
        
        # –§–∏–ª—å—Ç—Ä: –Ω–æ–≤–∏–∑–Ω–∞
        if novelty_level:
            try:
                year = int(paper.get("published", "2000"))
                current_year = 2025
                age = current_year - year
                
                if novelty_level == "high" and age > 3:
                    continue
                elif novelty_level == "low" and age < 5:
                    continue
            except:
                pass
        
        # –§–∏–ª—å—Ç—Ä: –¥–æ–º–µ–Ω (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        if domain:
            venue = paper.get("venue", "").lower()
            categories = paper.get("categories", [])
            categories_str = " ".join(categories).lower() if isinstance(categories, list) else ""
            
            if domain == "medicine":
                medical_keywords = ["medical", "clinical", "biomedical", "pubmed", "nejm", "lancet", "jama"]
                if not any(kw in venue or kw in categories_str for kw in medical_keywords):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ source
                    if paper.get("source") != "pubmed":
                        continue
            
            elif domain == "cs":
                cs_keywords = ["computer", "ai", "machine learning", "neural", "algorithm", "arxiv", "neurips", "icml"]
                if not any(kw in venue or kw in categories_str for kw in cs_keywords):
                    if paper.get("source") == "pubmed":
                        continue
        
        filtered.append(paper)
    
    return filtered


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 10: Deduper/Normalizer - –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
# ============================================================================

def deduplicate_and_normalize(state: AgentState) -> AgentState:
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ DOI/ArXiv ID/–Ω–∞–∑–≤–∞–Ω–∏—é (fuzzy match)
    –∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """
    print("\nüßπ –§—É–Ω–∫—Ü–∏—è 10: Deduper/Normalizer - –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    
    seed_results = state["seed_results"]
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    seen_identifiers = {}
    corpus_index = []
    
    from difflib import SequenceMatcher
    
    def normalize_title(title):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        import re
        title = title.lower()
        title = re.sub(r'[^\w\s]', '', title)
        title = re.sub(r'\s+', ' ', title)
        return title.strip()
    
    def is_similar_title(title1, title2, threshold=0.85):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π"""
        norm1 = normalize_title(title1)
        norm2 = normalize_title(title2)
        ratio = SequenceMatcher(None, norm1, norm2).ratio()
        return ratio >= threshold
    
    duplicates_count = 0
    
    for paper in seed_results:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ DOI
        doi = paper.get("doi")
        if doi and doi in seen_identifiers:
            duplicates_count += 1
            # –û–±–æ–≥–∞—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
            existing = seen_identifiers[doi]
            if paper.get("citations_total", 0) > existing.get("citations_total", 0):
                existing["citations_total"] = paper["citations_total"]
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ ArXiv ID
        arxiv_id = paper.get("arxiv_id")
        if arxiv_id and arxiv_id in seen_identifiers:
            duplicates_count += 1
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ PMID
        pmid = paper.get("pmid")
        if pmid and pmid in seen_identifiers:
            duplicates_count += 1
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (fuzzy match)
        title = paper.get("title", "")
        is_duplicate = False
        
        for existing_paper in corpus_index[-20:]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            if is_similar_title(title, existing_paper.get("title", "")):
                duplicates_count += 1
                is_duplicate = True
                break
        
        if is_duplicate:
            continue
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é
        normalized_paper = {
            **paper,
            "normalized_title": normalize_title(title),
            "citations_per_year": 0,
            "recency_score": 0,
            "relevance_score": 0
        }
        
        # –í—ã—á–∏—Å–ª—è–µ–º citations_per_year
        try:
            year = int(paper.get("published", "2020"))
            years_since = max(2025 - year, 1)
            normalized_paper["citations_per_year"] = paper.get("citations_total", 0) / years_since
        except:
            pass
        
        # –í—ã—á–∏—Å–ª—è–µ–º recency_score (–±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ = –≤—ã—à–µ)
        try:
            year = int(paper.get("published", "2000"))
            normalized_paper["recency_score"] = max(0, (year - 2000) / 25.0)  # 0 to 1
        except:
            normalized_paper["recency_score"] = 0
        
        corpus_index.append(normalized_paper)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        if doi:
            seen_identifiers[doi] = normalized_paper
        if arxiv_id:
            seen_identifiers[arxiv_id] = normalized_paper
        if pmid:
            seen_identifiers[pmid] = normalized_paper
    
    state["corpus_index"] = corpus_index
    state["next_step"] = "rank"
    
    print(f"   ‚úì –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count}")
    print(f"   ‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –∫–æ—Ä–ø—É—Å–µ: {len(corpus_index)}")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 11: Ranker - –≥–∏–±—Ä–∏–¥–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π
# ============================================================================

def hybrid_ranker(state: AgentState) -> AgentState:
    """
    –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ä–∞–Ω–∫–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ part_1:
    - Semantic search (dense embeddings) - –û–°–ù–û–í–ù–û–ô –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–≤–µ—Å 0.75)
    - –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –∏ –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∞—Ç—å/–≤—ã–∫–ª—é—á–∞—Ç—å —á–µ—Ä–µ–∑ config
    
    –§–æ—Ä–º—É–ª–∞ (—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤–µ—Å–æ–≤):
    score = w_semantic * semantic_score + [–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã]
    """
    print("\nüìä –§—É–Ω–∫—Ü–∏—è 11: Simplified Ranker - —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π...")
    print("   üí° –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ part_1: semantic search - –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥")
    
    import math
    from collections import Counter
    import re
    import numpy as np
    
    corpus = state["corpus_index"]
    query = state["query"]
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    try:
        from config import (
            EXPERIMENTAL, VECTOR_SEARCH_CONFIG, 
            RANKING_COMPONENTS, RANKING_WEIGHTS
        )
        use_vector_search = (
            EXPERIMENTAL.get("enable_semantic_search", True) and 
            RANKING_COMPONENTS.get("semantic_search", True)
        )
        vec_weight = VECTOR_SEARCH_CONFIG.get("weight", 0.75)
        batch_size = VECTOR_SEARCH_CONFIG.get("batch_size", 100)
        embedding_model = VECTOR_SEARCH_CONFIG.get("embedding_model", "text-embedding-3-small")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤–∫–ª—é—á–µ–Ω—ã
        use_recency = RANKING_COMPONENTS.get("recency", True)
        use_cpy = RANKING_COMPONENTS.get("citations_per_year", True)
        use_ctotal = RANKING_COMPONENTS.get("citations_total", True)
        use_venue = RANKING_COMPONENTS.get("venue", True)
        use_keywords = RANKING_COMPONENTS.get("keywords_bm25", False)
        
        # –ë–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        w_vec = vec_weight if use_vector_search else 0.0
        w_recency = RANKING_WEIGHTS.get("recency", 0.10) if use_recency else 0.0
        w_cpy = RANKING_WEIGHTS.get("cpy", 0.08) if use_cpy else 0.0
        w_ctotal = RANKING_WEIGHTS.get("ctotal", 0.05) if use_ctotal else 0.0
        w_venue = RANKING_WEIGHTS.get("venue", 0.02) if use_venue else 0.0
        w_kw = RANKING_WEIGHTS.get("keywords", 0.00) if use_keywords else 0.0
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        use_vector_search = True
        vec_weight = 0.75
        batch_size = 100
        embedding_model = "text-embedding-3-small"
        use_recency = use_cpy = use_ctotal = use_venue = True
        use_keywords = False
        w_vec = 0.75
        w_recency = 0.10
        w_cpy = 0.08
        w_ctotal = 0.05
        w_venue = 0.02
        w_kw = 0.00
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞ (—Å—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0)
    total_weight = w_vec + w_recency + w_cpy + w_ctotal + w_venue + w_kw
    if total_weight > 0:
        w_vec /= total_weight
        w_recency /= total_weight
        w_cpy /= total_weight
        w_ctotal /= total_weight
        w_venue /= total_weight
        w_kw /= total_weight
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∫–ª—é—á—ë–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
    enabled_components = []
    if use_vector_search:
        enabled_components.append(f"semantic ({w_vec:.1%})")
    if use_recency:
        enabled_components.append(f"recency ({w_recency:.1%})")
    if use_cpy:
        enabled_components.append(f"citations/year ({w_cpy:.1%})")
    if use_ctotal:
        enabled_components.append(f"citations/total ({w_ctotal:.1%})")
    if use_venue:
        enabled_components.append(f"venue ({w_venue:.1%})")
    if use_keywords:
        enabled_components.append(f"keywords ({w_kw:.1%})")
    
    print(f"   üìã –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {', '.join(enabled_components)}")
    
    # Venue scores (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
    venue_scores = {
        "nature": 1.0, "science": 1.0, "cell": 1.0,
        "nejm": 0.95, "lancet": 0.95, "jama": 0.95,
        "neurips": 0.90, "icml": 0.90, "iclr": 0.90, "cvpr": 0.90,
        "acl": 0.85, "emnlp": 0.85, "naacl": 0.85
    }
    
    def get_venue_score(venue):
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É venue"""
        if not venue or venue == "Unknown":
            return 0.3
        venue_lower = venue.lower()
        for key, score in venue_scores.items():
            if key in venue_lower:
                return score
        return 0.5  # Default –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö venues
    
    def simple_bm25(query_text, document_text):
        """–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π BM25 –¥–ª—è keyword matching"""
        query_tokens = set(re.findall(r'\w+', query_text.lower()))
        doc_tokens = re.findall(r'\w+', document_text.lower())
        doc_freq = Counter(doc_tokens)
        
        score = 0
        for token in query_tokens:
            if token in doc_freq:
                tf = doc_freq[token]
                score += (tf / (tf + 1.5)) * 2.0
        
        return score / max(len(query_tokens), 1)
    
    # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)
    query_embedding = None
    if use_vector_search:
        try:
            print("   üîç –°–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
            embeddings = get_embeddings()
            query_embedding = np.array(embeddings.embed_query(query))
            print(f"   ‚úì Embeddings —Å–æ–∑–¥–∞–Ω—ã (–º–æ–¥–µ–ª—å: {embedding_model})")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è embeddings: {e}")
            use_vector_search = False
            w_vec = 0.0
            w_kw = 0.20
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —Ä–∞—Å—Å—á—ë—Ç–∞
    max_cpy = max([p.get("citations_per_year", 0) for p in corpus] + [1])
    max_ctotal = max([p.get("citations_total", 0) for p in corpus] + [1])
    
    # –°–æ–∑–¥–∞—ë–º FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)
    faiss_index = None
    doc_embeddings_list = []
    if use_vector_search and query_embedding is not None:
        try:
            print("   üìä –°–æ–∑–¥–∞–Ω–∏–µ embeddings –∏ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            embeddings = get_embeddings()
            
            # –°–æ–∑–¥–∞—ë–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è embedding (title + summary)
            doc_texts = []
            for paper in corpus:
                doc_text = f"{paper.get('title', '')} {paper.get('summary', '')[:500]}"
                doc_texts.append(doc_text)
            
            # –°–æ–∑–¥–∞—ë–º embeddings –±–∞—Ç—á–∞–º–∏ (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã)
            for i in range(0, len(doc_texts), batch_size):
                batch = doc_texts[i:i+batch_size]
                batch_embeddings = embeddings.embed_documents(batch)
                doc_embeddings_list.extend(batch_embeddings)
            
            # –°–æ–∑–¥–∞—ë–º FAISS –∏–Ω–¥–µ–∫—Å
            import faiss
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
            dimension = len(doc_embeddings_list[0])
            
            # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å FAISS (L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å InnerProduct –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º IndexFlatIP (Inner Product) –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ (L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
            doc_embeddings_array = np.array(doc_embeddings_list).astype('float32')
            faiss.normalize_L2(doc_embeddings_array)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –≤ –∏–Ω–¥–µ–∫—Å
            faiss_index.add(doc_embeddings_array)
            
            print(f"   ‚úì –°–æ–∑–¥–∞–Ω FAISS –∏–Ω–¥–µ–∫—Å —Å {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension})")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            use_vector_search = False
            w_vec = 0.0
            w_kw = 0.20
            faiss_index = None
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ FAISS (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ) –ü–ï–†–ï–î —Ä–∞—Å—á–µ—Ç–æ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
    vec_scores_dict = {}
    if use_vector_search and faiss_index is not None and query_embedding is not None:
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º query embedding –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            query_vec = np.array([query_embedding]).astype('float32')
            faiss.normalize_L2(query_vec)
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (k = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
            k = min(len(corpus), faiss_index.ntotal)
            distances, indices = faiss_index.search(query_vec, k)
            
            # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å—Ö–æ–¥—Å—Ç–≤–∞–º
            for idx, distance in zip(indices[0], distances[0]):
                if idx < len(corpus):
                    # Inner Product –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ = –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç [-1, 1] –∫ [0, 1]
                    vec_scores_dict[idx] = (distance + 1) / 2
            
            print(f"   ‚úì –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω—ã —á–µ—Ä–µ–∑ FAISS")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ FAISS: {e}")
            vec_scores_dict = {}
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
    for i, paper in enumerate(corpus):
        # Recency score —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω
        recency = paper.get("recency_score", 0)
        
        # Citations per year (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)
        cpy = paper.get("citations_per_year", 0) / max_cpy if max_cpy > 0 else 0
        
        # Total citations (log scale, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)
        ctotal = paper.get("citations_total", 0)
        log_ctotal = math.log1p(ctotal) / math.log1p(max_ctotal) if max_ctotal > 0 else 0
        
        # Venue score
        venue = paper.get("venue", "")
        venue_score = get_venue_score(venue)
        
        # Keyword matching (BM25-like) - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        kw_score = 0.0
        if use_keywords:
            doc_text = f"{paper.get('title', '')} {paper.get('summary', '')}"
            kw_score = simple_bm25(query.lower(), doc_text)
        
        # Vector similarity –∏–∑ FAISS - –û–°–ù–û–í–ù–û–ô –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        vec_score = vec_scores_dict.get(i, 0.0) if use_vector_search else 0.0
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ (semantic search - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç)
        final_score = w_vec * vec_score
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if use_recency:
            final_score += w_recency * recency
        if use_cpy:
            final_score += w_cpy * cpy
        if use_ctotal:
            final_score += w_ctotal * log_ctotal
        if use_venue:
            final_score += w_venue * venue_score
        if use_keywords:
            final_score += w_kw * kw_score
        
        # ============================================================================
        # –§–ê–ó–ê 4: –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–ó–∞–¥–∞—á–∞ 4.1)
        # ============================================================================
        source_weight = paper.get("_source_weight", 1.0)
        final_score = final_score * source_weight
        
        paper["relevance_score"] = final_score
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ü–µ–Ω–∫–∏ (—Ç–æ–ª—å–∫–æ –≤–∫–ª—é—á—ë–Ω–Ω—ã–µ)
        score_components = {}
        if use_vector_search:
            score_components["semantic"] = vec_score * w_vec
        if use_recency:
            score_components["recency"] = recency * w_recency
        if use_cpy:
            score_components["citations_per_year"] = cpy * w_cpy
        if use_ctotal:
            score_components["citations_total"] = log_ctotal * w_ctotal
        if use_venue:
            score_components["venue"] = venue_score * w_venue
        if use_keywords:
            score_components["keywords"] = kw_score * w_kw
        
        paper["score_components"] = score_components
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ
    ranked_papers = sorted(corpus, key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # –ë–µ—Ä—ë–º —Ç–æ–ø-N –¥–ª—è reranking
    top_n = min(state.get("max_papers", 40), len(ranked_papers))
    top_papers_for_rerank = ranked_papers[:top_n]
    
    # LLM-rerank –ø–æ–≤–µ—Ä—Ö —Ç–æ–ø-N (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
    try:
        from config import EXPERIMENTAL, RERANK_CONFIG
        enable_rerank = EXPERIMENTAL.get("enable_llm_rerank", False)
        rerank_top_k = RERANK_CONFIG.get("top_k", 20)
    except:
        enable_rerank = False
        rerank_top_k = 20
    
    if enable_rerank and len(top_papers_for_rerank) > 5:
        print(f"   üîÑ LLM-rerank –¥–ª—è —Ç–æ–ø-{min(rerank_top_k, len(top_papers_for_rerank))} —Å—Ç–∞—Ç–µ–π...")
        reranked_papers = llm_rerank(
            query=query,
            papers=top_papers_for_rerank[:rerank_top_k],
            llm=get_llm()
        )
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ reranked_papers –Ω–µ None
        if reranked_papers:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º reranked —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏
            reranked_indices = {p.get("_original_index") for p in reranked_papers if "_original_index" in p}
            final_ranked = reranked_papers + [p for i, p in enumerate(top_papers_for_rerank) if i not in reranked_indices]
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–ª—è
            for p in final_ranked:
                p.pop("_original_index", None)
            state["ranked_papers"] = final_ranked[:top_n]
        else:
            # –ï—Å–ª–∏ rerank –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            state["ranked_papers"] = top_papers_for_rerank
    else:
        state["ranked_papers"] = top_papers_for_rerank
    
    # Citation snowballing (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
    try:
        from config import EXPERIMENTAL, SNOWBALL_CONFIG
        if EXPERIMENTAL.get("enable_snowballer", False) and SNOWBALL_CONFIG.get("enabled", False):
            print(f"   üîó Citation snowballing...")
            expanded_papers = citation_snowballing(
                seed_papers=state["ranked_papers"][:10],  # –¢–æ–ø-10 –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                max_expansion=SNOWBALL_CONFIG.get("max_expansion", 20),
                min_citations=SNOWBALL_CONFIG.get("min_citations", 5),
                forward=SNOWBALL_CONFIG.get("forward_citations", True),
                backward=SNOWBALL_CONFIG.get("backward_citations", True)
            )
            if expanded_papers:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –∫–æ—Ä–ø—É—Å
                existing_ids = {p.get("doi") or p.get("arxiv_id") or p.get("url") for p in state["ranked_papers"]}
                new_papers = [p for p in expanded_papers if (p.get("doi") or p.get("arxiv_id") or p.get("url")) not in existing_ids]
                if new_papers:
                    state["ranked_papers"].extend(new_papers[:SNOWBALL_CONFIG.get("max_expansion", 20)])
                    print(f"      ‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_papers)} —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ citation snowballing")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ citation snowballing: {e}")
    
    state["next_step"] = "read_pdfs"
    
    print(f"   ‚úì –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(ranked_papers)}")
    print(f"   ‚úì –í—ã–±—Ä–∞–Ω–æ —Ç–æ–ø-{top_n} –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 —Å –∏—Ö –æ—Ü–µ–Ω–∫–∞–º–∏
    print("\n   üèÜ –¢–æ–ø-3 —Å—Ç–∞—Ç—å–∏:")
    for i, paper in enumerate(ranked_papers[:3], 1):
        print(f"      {i}. {paper.get('title', 'No title')[:60]}...")
        print(f"         Score: {paper['relevance_score']:.3f} | –ì–æ–¥: {paper.get('published')} | "
              f"–¶–∏—Ç.: {paper.get('citations_total', 0)}")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–ò: LLM-rerank –∏ Citation Snowballing
# ============================================================================

def llm_rerank(query: str, papers: List[Dict[str, Any]], llm) -> Optional[List[Dict[str, Any]]]:
    """
    LLM-rerank: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–ø-N —Å—Ç–∞—Ç–µ–π
    –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ cross-encoder, —Å—Ä–∞–≤–Ω–∏–≤–∞—è –∑–∞–ø—Ä–æ—Å —Å –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å—ë–π
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
    """
    if len(papers) == 0:
        return papers
    
    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∫ –∑–∞–ø—Ä–æ—Å—É.

–û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ –∫ –∑–∞–ø—Ä–æ—Å—É –ø–æ —à–∫–∞–ª–µ –æ—Ç 0.0 –¥–æ 1.0, –≥–¥–µ:
- 1.0 = –∏–¥–µ–∞–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—Å—É
- 0.8-0.9 = –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
- 0.6-0.7 = —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
- 0.4-0.5 = —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
- 0.0-0.3 = –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞

–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏:
[
  {"index": 0, "relevance_score": 0.95, "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"},
  {"index": 1, "relevance_score": 0.87, "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"},
  ...
]
"""
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∞—Ç—å—è—Ö
    papers_info = []
    for i, paper in enumerate(papers):
        papers_info.append(f"""
–°—Ç–∞—Ç—å—è {i}:
–ù–∞–∑–≤–∞–Ω–∏–µ: {paper.get('title', 'Unknown')}
–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è: {paper.get('summary', 'No abstract')[:300]}
–ì–æ–¥: {paper.get('published')}
–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {paper.get('citations_total', 0)}
""")
    
    user_prompt = f"""–ó–∞–ø—Ä–æ—Å: {query}

–°—Ç–∞—Ç—å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏:
{chr(10).join(papers_info)}

–û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ –∫ –∑–∞–ø—Ä–æ—Å—É."""
    
    try:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = llm.invoke(messages)
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        parsed = safe_json_parse(response.content, [])
        
        if not isinstance(parsed, list) or len(parsed) == 0:
            print(f"      ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã LLM-rerank")
            return None
        
        # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –æ—Ü–µ–Ω–æ–∫
        rerank_scores = {}
        for item in parsed:
            if isinstance(item, dict) and "index" in item and "relevance_score" in item:
                idx = item["index"]
                score = float(item.get("relevance_score", 0.5))
                rerank_scores[idx] = score
        
        # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –Ω–æ–≤—ã–º –æ—Ü–µ–Ω–∫–∞–º
        reranked = []
        for i, paper in enumerate(papers):
            new_score = rerank_scores.get(i, paper.get("relevance_score", 0.5))
            paper_copy = paper.copy()
            paper_copy["rerank_score"] = new_score
            paper_copy["relevance_score"] = new_score  # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–π score
            paper_copy["_original_index"] = i
            reranked.append(paper_copy)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤—ã–º –æ—Ü–µ–Ω–∫–∞–º
        reranked.sort(key=lambda x: x.get("rerank_score", 0), reverse=True)
        
        print(f"      ‚úì LLM-rerank –∑–∞–≤–µ—Ä—à—ë–Ω –¥–ª—è {len(reranked)} —Å—Ç–∞—Ç–µ–π")
        return reranked
    
    except Exception as e:
        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ LLM-rerank: {e}")
        return None
    
    except Exception as e:
        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM-rerank: {e}")
        return papers


def citation_snowballing(
    seed_papers: List[Dict[str, Any]],
    max_expansion: int = 20,
    min_citations: int = 5,
    forward: bool = True,
    backward: bool = True
) -> List[Dict[str, Any]]:
    """
    –†–∞—Å—à–∏—Ä—è–µ—Ç –∫–æ—Ä–ø—É—Å —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ citation snowballing:
    - Forward citations: –∫—Ç–æ —Ü–∏—Ç–∏—Ä—É–µ—Ç seed papers
    - Backward citations: –Ω–∞ –∫–æ–≥–æ —Å—Å—ã–ª–∞—é—Ç—Å—è seed papers
    """
    expanded_papers = []
    
    try:
        import requests
        import time
        
        for paper in seed_papers[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            paper_id = paper.get("doi") or paper.get("arxiv_id")
            if not paper_id:
                continue
            
            # Forward citations —á–µ—Ä–µ–∑ Semantic Scholar
            if forward:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Semantic Scholar API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è citations
                    if paper.get("doi"):
                        s2_url = f"https://api.semanticscholar.org/graph/v1/paper/DOI:{paper['doi']}/citations"
                    elif paper.get("arxiv_id"):
                        s2_url = f"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper['arxiv_id']}/citations"
                    else:
                        continue
                    
                    response = requests.get(
                        s2_url,
                        params={"limit": 10, "fields": "title,authors,abstract,year,citationCount,url,externalIds"},
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        for cited_paper in data.get("data", [])[:5]:
                            if cited_paper.get("citationCount", 0) >= min_citations:
                                expanded_papers.append({
                                    "title": cited_paper.get("title", ""),
                                    "authors": [a.get("name", "") for a in cited_paper.get("authors", [])],
                                    "summary": cited_paper.get("abstract", "")[:500],
                                    "url": cited_paper.get("url", ""),
                                    "doi": cited_paper.get("externalIds", {}).get("DOI"),
                                    "published": str(cited_paper.get("year", "Unknown")),
                                    "citations_total": cited_paper.get("citationCount", 0),
                                    "source": "semantic_scholar",
                                    "_snowball_type": "forward"
                                })
                    
                    time.sleep(0.1)
                except Exception as e:
                    pass
            
            # Backward citations (references)
            if backward and len(expanded_papers) < max_expansion:
                try:
                    if paper.get("doi"):
                        s2_url = f"https://api.semanticscholar.org/graph/v1/paper/DOI:{paper['doi']}/references"
                    elif paper.get("arxiv_id"):
                        s2_url = f"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper['arxiv_id']}/references"
                    else:
                        continue
                    
                    response = requests.get(
                        s2_url,
                        params={"limit": 10, "fields": "title,authors,abstract,year,citationCount,url,externalIds"},
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        for ref_paper in data.get("data", [])[:5]:
                            if ref_paper.get("citationCount", 0) >= min_citations:
                                expanded_papers.append({
                                    "title": ref_paper.get("title", ""),
                                    "authors": [a.get("name", "") for a in ref_paper.get("authors", [])],
                                    "summary": ref_paper.get("abstract", "")[:500],
                                    "url": ref_paper.get("url", ""),
                                    "doi": ref_paper.get("externalIds", {}).get("DOI"),
                                    "published": str(ref_paper.get("year", "Unknown")),
                                    "citations_total": ref_paper.get("citationCount", 0),
                                    "source": "semantic_scholar",
                                    "_snowball_type": "backward"
                                })
                    
                    time.sleep(0.1)
                except Exception as e:
                    pass
            
            if len(expanded_papers) >= max_expansion:
                break
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        seen_ids = set()
        unique_papers = []
        for p in expanded_papers:
            paper_id = p.get("doi") or p.get("arxiv_id") or p.get("url")
            if paper_id and paper_id not in seen_ids:
                seen_ids.add(paper_id)
                unique_papers.append(p)
        
        return unique_papers[:max_expansion]
    
    except Exception as e:
        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ citation snowballing: {e}")
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 12: PDF Reader - —á—Ç–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Å—Å—ã–ª–∫–∞–º
# ============================================================================

def get_pdf_url_from_paper(paper: Dict[str, Any]) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    """
    # 1. –î–ª—è ArXiv - —Å—Ç—Ä–æ–∏–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ PDF
    arxiv_id = paper.get("arxiv_id")
    if arxiv_id:
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –µ—Å–ª–∏ –µ—Å—Ç—å
        arxiv_clean = arxiv_id.replace("arxiv:", "").replace("arXiv:", "").strip()
        return f"https://arxiv.org/pdf/{arxiv_clean}.pdf"
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É (–º–æ–∂–µ—Ç –±—ã—Ç—å PDF)
    url = paper.get("url", "")
    if url and url.endswith(".pdf"):
        return url
    
    # 3. –î–ª—è DOI - –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∂—É—Ä–Ω–∞–ª—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç PDF —á–µ—Ä–µ–∑ DOI
    # –ù–æ —ç—Ç–æ —Å–ª–æ–∂–Ω–µ–µ, –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    
    return None


def extract_pdf_sections(pdf_text: str) -> Dict[str, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –∏–∑ PDF —Ç–µ–∫—Å—Ç–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç regex –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–µ–∫—Ü–∏–π
    """
    import re
    
    sections = {
        "introduction": "",
        "methods": "",
        "results": "",
        "discussion": "",
        "conclusion": "",
        "related_work": "",
        "limitations": "",
        "future_work": "",
        "contributions": "",
        "experimental_setup": ""
    }
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–µ–∫—Ü–∏–π
    section_patterns = {
        "introduction": [
            r'##?\s*\d*\.?\s*Introduction',
            r'##?\s*\d*\.?\s*Background',
            r'##?\s*\d*\.?\s*Motivation',
        ],
        "methods": [
            r'##?\s*\d*\.?\s*Methods?',
            r'##?\s*\d*\.?\s*Methodology',
            r'##?\s*\d*\.?\s*Approach',
            r'##?\s*\d*\.?\s*Algorithm',
        ],
        "results": [
            r'##?\s*\d*\.?\s*Results?',
            r'##?\s*\d*\.?\s*Experiments?',
            r'##?\s*\d*\.?\s*Evaluation',
        ],
        "discussion": [
            r'##?\s*\d*\.?\s*Discussion',
            r'##?\s*\d*\.?\s*Analysis',
        ],
        "conclusion": [
            r'##?\s*\d*\.?\s*Conclusion',
            r'##?\s*\d*\.?\s*Conclusions?',
            r'##?\s*\d*\.?\s*Summary',
        ],
        "related_work": [
            r'##?\s*\d*\.?\s*Related\s+Work',
            r'##?\s*\d*\.?\s*Related\s+Literature',
            r'##?\s*\d*\.?\s*Background\s+and\s+Related\s+Work',
        ],
        "limitations": [
            r'##?\s*\d*\.?\s*Limitations?',
            r'##?\s*\d*\.?\s*Discussion\s+of\s+Limitations',
            r'##?\s*\d*\.?\s*Threats\s+to\s+Validity',
        ],
        "future_work": [
            r'##?\s*\d*\.?\s*Future\s+(Work|Directions|Research)',
            r'##?\s*\d*\.?\s*Open\s+Problems?',
        ],
        "contributions": [
            r'##?\s*\d*\.?\s*Contributions?',
            r'##?\s*\d*\.?\s*Main\s+Contributions?',
        ],
        "experimental_setup": [
            r'##?\s*\d*\.?\s*Experimental\s+Setup',
            r'##?\s*\d*\.?\s*Implementation',
            r'##?\s*\d*\.?\s*Experimental\s+Settings?',
        ]
    }
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–µ–∫—Ü–∏–∏
    found_sections = {}
    for section_name, patterns in section_patterns.items():
        for pattern in patterns:
            match = re.search(pattern, pdf_text, re.IGNORECASE)
            if match:
                section_start = match.start()
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å–µ–∫—Ü–∏—é –∏–ª–∏ –∫–æ–Ω–µ—Ü
                next_section_match = re.search(
                    r'\n##?\s*\d+\.',
                    pdf_text[section_start + 10:]
                )
                section_end = (
                    section_start + 10 + next_section_match.start()
                    if next_section_match
                    else len(pdf_text)
                )
                found_sections[section_name] = pdf_text[section_start:section_end]
                break
    
    return found_sections


def download_and_parse_pdf(url: str) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç PDF –ø–æ —Å—Å—ã–ª–∫–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        import requests
        import pdfplumber
        import io
        
        # –°–∫–∞—á–∏–≤–∞–µ–º PDF
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º content-type
        content_type = response.headers.get("content-type", "").lower()
        if "pdf" not in content_type and not url.endswith(".pdf"):
            return None
        
        pdf_content = response.content
        
        # –ü–∞—Ä—Å–∏–º PDF
        text_parts = []
        with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            max_pages = min(50, len(pdf.pages))
            for page in pdf.pages[:max_pages]:
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)
                except:
                    continue
        
        full_text = "\n\n".join(text_parts)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤)
        max_chars = 50000
        if len(full_text) > max_chars:
            full_text = full_text[:max_chars] + "\n\n[... —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω ...]"
        
        return full_text
    
    except Exception as e:
        # Fallback –Ω–∞ PyMuPDF –µ—Å–ª–∏ pdfplumber –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        try:
            import fitz  # PyMuPDF
            import requests
            import io
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            doc = fitz.open(stream=response.content, filetype="pdf")
            text_parts = []
            for i in range(min(50, len(doc))):
                text_parts.append(doc[i].get_text())
            doc.close()
            
            full_text = "\n\n".join(text_parts)
            if len(full_text) > 50000:
                full_text = full_text[:50000] + "\n\n[... —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω ...]"
            
            return full_text
        
        except Exception as e2:
            return None


def read_pdfs(state: AgentState) -> AgentState:
    """
    –ß–∏—Ç–∞–µ—Ç –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã PDF –¥–ª—è —Ç–æ–ø-—Å—Ç–∞—Ç–µ–π –ø–æ –ø—Ä—è–º—ã–º —Å—Å—ã–ª–∫–∞–º
    """
    print("\nüìÑ –§—É–Ω–∫—Ü–∏—è 12: PDF Reader - —á—Ç–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤...")
    
    ranked_papers = state.get("ranked_papers", [])
    
    # –ë–µ—Ä—ë–º —Ç–æ–ø-5 —Å—Ç–∞—Ç–µ–π –¥–ª—è —á—Ç–µ–Ω–∏—è PDF (—á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
    top_for_pdf = min(5, len(ranked_papers))
    papers_to_read = ranked_papers[:top_for_pdf]
    
    print(f"   üì• –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF –¥–ª—è —Ç–æ–ø-{top_for_pdf} —Å—Ç–∞—Ç–µ–π...")
    
    pdf_texts = {}
    successful = 0
    
    for i, paper in enumerate(papers_to_read, 1):
        title = paper.get("title", "Unknown")[:60]
        print(f"      [{i}/{top_for_pdf}] {title}...")
        
        # –ü–æ–ª—É—á–∞–µ–º URL –¥–ª—è PDF
        pdf_url = get_pdf_url_from_paper(paper)
        
        if not pdf_url:
            print(f"         ‚ö†Ô∏è  –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF")
            continue
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º PDF
        pdf_text = download_and_parse_pdf(pdf_url)
        
        if not pdf_text or len(pdf_text) < 100:
            print(f"         ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF")
            continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
        paper_id = paper.get("doi") or paper.get("arxiv_id") or paper.get("url", f"paper_{i}")
        pdf_texts[paper_id] = pdf_text
        successful += 1
        print(f"         ‚úì PDF –ø—Ä–æ—á–∏—Ç–∞–Ω ({len(pdf_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    state["pdf_texts"] = pdf_texts
    state["next_step"] = "summarize"
    
    print(f"\n   ‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ PDF: {successful}/{top_for_pdf}")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–ò: –ü–æ–∏—Å–∫ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤
# ============================================================================

def search_github_repositories(paper: Dict[str, Any], max_results: int = 3) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç–∞—Ç—å—ë–π —á–µ—Ä–µ–∑ GitHub API
    """
    try:
        from config import GITHUB_CONFIG
        if not GITHUB_CONFIG.get("enabled", True) or not GITHUB_CONFIG.get("use_api", True):
            return []
    except:
        pass
    
    import requests
    import os
    
    github_token = os.getenv("GITHUB_TOKEN")
    headers = {"Accept": "application/vnd.github.v3+json"}
    if github_token:
        headers["Authorization"] = f"token {github_token}"
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    title = paper.get("title", "")
    authors = paper.get("authors", [])
    first_author = authors[0].split()[-1] if authors else ""
    
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞
    search_queries = []
    if title:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
        keywords = title.split()[:5]  # –ü–µ—Ä–≤—ã–µ 5 —Å–ª–æ–≤
        search_queries.append(" ".join(keywords))
    if first_author:
        search_queries.append(first_author)
    
    repos = []
    for query in search_queries[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 –∑–∞–ø—Ä–æ—Å–∞
        try:
            url = "https://api.github.com/search/repositories"
            params = {
                "q": f"{query} in:name,description,readme",
                "sort": "stars",
                "order": "desc",
                "per_page": max_results
            }
            
            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for repo in data.get("items", []):
                    repos.append({
                        "name": repo.get("full_name", ""),
                        "url": repo.get("html_url", ""),
                        "stars": repo.get("stargazers_count", 0),
                        "forks": repo.get("forks_count", 0),
                        "language": repo.get("language", ""),
                        "description": repo.get("description", ""),
                        "updated_at": repo.get("updated_at", "")
                    })
                break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫
        except Exception as e:
            continue
    
    return repos[:max_results]


def web_search_github(paper: Dict[str, Any], max_results: int = 3) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫ (DuckDuckGo) –∫–∞–∫ fallback
    """
    try:
        from config import GITHUB_CONFIG
        if not GITHUB_CONFIG.get("enabled", True) or not GITHUB_CONFIG.get("use_web_search", True):
            return []
    except:
        pass
    
    try:
        from duckduckgo_search import DDGS
        
        title = paper.get("title", "")
        authors = paper.get("authors", [])
        first_author = authors[0] if authors else ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        search_query = f"{title} github" if title else f"{first_author} github"
        
        repos = []
        with DDGS() as ddgs:
            results = ddgs.text(
                search_query,
                max_results=max_results * 2  # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ, –ø–æ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            )
            
            for result in results:
                url = result.get("href", "")
                if "github.com" in url:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ URL
                    parts = url.replace("https://github.com/", "").replace("http://github.com/", "").split("/")
                    if len(parts) >= 2:
                        repos.append({
                            "name": f"{parts[0]}/{parts[1]}",
                            "url": url,
                            "stars": 0,  # –ù–µ –¥–æ—Å—Ç—É–ø–Ω–æ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫
                            "forks": 0,
                            "language": "",
                            "description": result.get("body", "")[:200],
                            "updated_at": ""
                        })
                        if len(repos) >= max_results:
                            break
        
        return repos
    except ImportError:
        # DuckDuckGo –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        return []
    except Exception as e:
        return []


def find_github_for_paper(paper: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ GitHub: —Å–Ω–∞—á–∞–ª–∞ API, –ø–æ—Ç–æ–º –≤–µ–±-–ø–æ–∏—Å–∫
    """
    repos = []
    
    # –ü—Ä–æ–±—É–µ–º GitHub API
    repos = search_github_repositories(paper)
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ API, –ø—Ä–æ–±—É–µ–º –≤–µ–±-–ø–æ–∏—Å–∫
    if not repos:
        repos = web_search_github(paper)
    
    return repos


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 13: Summarizer - —Å–æ–∑–¥–∞–Ω–∏–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
# ============================================================================

def create_literature_matrix(state: AgentState) -> AgentState:
    """
    –°–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã —Ç–æ–ø-—Å—Ç–∞—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç PDF –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    """
    print("\nüìù –§—É–Ω–∫—Ü–∏—è 13: Summarizer - —Å–æ–∑–¥–∞–Ω–∏–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã...")
    
    ranked_papers = state["ranked_papers"]
    pdf_texts = state.get("pdf_texts", {})
    llm = get_llm()
    lit_matrix = []
    
    # –ë–µ—Ä—ë–º —Ç–æ–ø-10 –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤)
    top_papers = ranked_papers[:min(10, len(ranked_papers))]
    
    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.
    
–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏—è, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç.
–ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç PDF, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

–í–ê–ñ–ù–û: –ò–∑–≤–ª–µ–∫–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–µ–∫—Ü–∏–π —Å—Ç–∞—Ç—å–∏:
- Introduction: –ø—Ä–æ–±–ª–µ–º–∞, –º–æ—Ç–∏–≤–∞—Ü–∏—è, —Ü–µ–ª–∏
- Methods/Methodology: –º–µ—Ç–æ–¥—ã, –ø–æ–¥—Ö–æ–¥—ã, –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- Results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –º–µ—Ç—Ä–∏–∫–∏, —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
- Discussion: –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
- Conclusion: –≤—ã–≤–æ–¥—ã, –∏—Ç–æ–≥–∏
- Related Work: –æ–±–∑–æ—Ä —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç
- Contributions: –≤–∫–ª–∞–¥ —Ä–∞–±–æ—Ç—ã (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–µ–π –∏–ª–∏ –≤ Introduction)
- Experimental Setup: –¥–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- Limitations: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ Discussion –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–µ–π)
- Future Work: –±—É–¥—É—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ Conclusion –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–µ–π)

–û–±—Ä–∞—Ç–∏ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞:
- –£–ø–æ–º–∏–Ω–∞–Ω–∏—è GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤, –∫–æ–¥–∞, –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è reproducibility_info)
- –°—Å—ã–ª–∫–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç—ã, –∫–æ–¥, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
- –î–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON:
{
  "problem": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–∑ Introduction",
  "methods": ["–º–µ—Ç–æ–¥ 1", "–º–µ—Ç–æ–¥ 2"],
  "datasets": ["–¥–∞—Ç–∞—Å–µ—Ç 1"],
  "metrics": ["–º–µ—Ç—Ä–∏–∫–∞ 1"],
  "key_findings": "–∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ Results/Discussion",
  "limitations": "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
  "future_work": "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π",
  "contributions": "–æ—Å–Ω–æ–≤–Ω–æ–π –≤–∫–ª–∞–¥ —Ä–∞–±–æ—Ç—ã",
  "related_work_summary": "–∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç –∏–∑ Related Work",
  "experimental_setup": "–¥–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è, –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ",
  "reproducibility_info": "–Ω–∞–ª–∏—á–∏–µ –∫–æ–¥–∞/–¥–∞–Ω–Ω—ã—Ö: GitHub —Å—Å—ã–ª–∫–∏, dataset links, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–¥–∞",
  "discussion": "–æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π",
  "conclusion": "–≤—ã–≤–æ–¥—ã, –∏—Ç–æ–≥–∏ —Ä–∞–±–æ—Ç—ã"
}
"""
    
    print(f"   –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-{len(top_papers)} —Å—Ç–∞—Ç–µ–π...")
    
    for i, paper in enumerate(top_papers, 1):
        try:
            paper_id = paper.get("doi") or paper.get("arxiv_id") or paper.get("url", f"paper_{i}")
            pdf_text = pdf_texts.get(paper_id)
            has_full_text = pdf_text is not None
            
            paper_info = f"""
–ù–∞–∑–≤–∞–Ω–∏–µ: {paper.get('title', 'Unknown')}
–ê–≤—Ç–æ—Ä—ã: {', '.join(paper.get('authors', [])[:5])}
–ì–æ–¥: {paper.get('published')}
Venue: {paper.get('venue')}
–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è: {paper.get('summary', 'No abstract')[:800]}
"""
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç PDF, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ
            if has_full_text:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏ –∏–∑ PDF
                pdf_sections = extract_pdf_sections(pdf_text)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                sections_text = ""
                for section_name, section_text in pdf_sections.items():
                    if section_text:
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏
                        section_excerpt = section_text[:3000] if len(section_text) > 3000 else section_text
                        sections_text += f"\n\n=== {section_name.upper()} ===\n{section_excerpt}"
                
                # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                if not sections_text:
                    pdf_excerpt = pdf_text[:20000] if len(pdf_text) > 20000 else pdf_text
                    sections_text = f"\n\n=== –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ –°–¢–ê–¢–¨–ò (PDF) ===\n{pdf_excerpt}"
                paper_info += sections_text
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=paper_info)
            ]
            
            # ============================================================================
            # –£–õ–£–ß–®–ï–ù–ò–ï: Retry –ª–æ–≥–∏–∫–∞ —Å exponential backoff (–ó–∞–¥–∞—á–∞ 1.2)
            # ============================================================================
            response = None
            max_retries = 3
            retry_count = 0
            last_error = None
            
            while retry_count < max_retries:
                try:
                    response = llm.invoke(messages)
                    break  # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç–≤–µ—Ç
                except Exception as e:
                    retry_count += 1
                    last_error = e
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count  # Exponential backoff: 2, 4, 8 —Å–µ–∫—É–Ω–¥
                        print(f"         ‚ö†Ô∏è  Retry {retry_count}/{max_retries} after {wait_time}s: {str(e)[:50]}")
                        import time
                        time.sleep(wait_time)
                    else:
                        # –ò—Å—á–µ—Ä–ø–∞–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏
                        raise last_error
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
            parsed = safe_json_parse(response.content, {})
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ Pydantic
            try:
                summary_model = StructuredSummaryModel(**parsed)
                summary = summary_model.model_dump()
            except ValidationError:
                # Fallback –Ω–∞ —Å—ã—Ä–æ–π dict —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                summary = {
                    "problem": parsed.get("problem", ""),
                    "methods": parsed.get("methods", []),
                    "datasets": parsed.get("datasets", []),
                    "metrics": parsed.get("metrics", []),
                    "key_findings": parsed.get("key_findings", ""),
                    "limitations": parsed.get("limitations", ""),
                    "future_work": parsed.get("future_work", ""),
                    "contributions": parsed.get("contributions", ""),
                    "related_work_summary": parsed.get("related_work_summary", ""),
                    "experimental_setup": parsed.get("experimental_setup", ""),
                    "reproducibility_info": parsed.get("reproducibility_info", ""),
                    "discussion": parsed.get("discussion", ""),
                    "conclusion": parsed.get("conclusion", "")
                }
            except Exception as e:
                # ============================================================================
                # –£–õ–£–ß–®–ï–ù–ò–ï: Fallback –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (–ó–∞–¥–∞—á–∞ 1.2)
                # ============================================================================
                # –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å—Ç–∞—Ç—å–∏
                abstract = paper.get('summary', paper.get('abstract', ''))
                
                summary = {
                    "problem": abstract[:300] if abstract else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å",
                    "methods": [],
                    "datasets": [],
                    "metrics": [],
                    "key_findings": abstract if abstract else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏",
                    "limitations": "LLM –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è.",
                    "future_work": "",
                    "contributions": "",
                    "related_work_summary": "",
                    "experimental_setup": "",
                    "reproducibility_info": "",
                    "discussion": "",
                    "conclusion": ""
                }
                print(f"         ‚ö†Ô∏è  Fallback –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è —Å—Ç–∞—Ç—å–∏ {i}: {str(e)[:50]}")
            
            # –ò—â–µ–º GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–ª—è —Å—Ç–∞—Ç—å–∏
            github_repos = []
            if has_full_text or summary.get("reproducibility_info"):
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–¥–∞, –∏—â–µ–º GitHub
                try:
                    github_repos = find_github_for_paper(paper)
                    if github_repos:
                        print(f"         üîó –ù–∞–π–¥–µ–Ω–æ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤: {len(github_repos)}")
                except Exception as e:
                    pass
            
            lit_matrix.append({
                **paper,
                "structured_summary": summary,
                "has_full_text": has_full_text,
                "github_repos": github_repos
            })
            
            status_icon = "üìÑ" if has_full_text else "üìù"
            print(f"      {status_icon} {i}/{len(top_papers)}: {paper.get('title', 'Unknown')[:50]}...")
            
        except Exception as e:
            # ============================================================================
            # –£–õ–£–ß–®–ï–ù–ò–ï: Fallback –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ (–ó–∞–¥–∞—á–∞ 1.2)
            # ============================================================================
            print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç—å–∏ {i}: {e}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –∫–∞–∫ fallback
            abstract = paper.get('summary', paper.get('abstract', ''))
            
            lit_matrix.append({
                **paper,
                "structured_summary": {
                    "problem": abstract[:300] if abstract else "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                    "methods": [],
                    "datasets": [],
                    "metrics": [],
                    "key_findings": abstract if abstract else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å",
                    "limitations": f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ LLM: {str(e)[:100]}",
                    "future_work": "",
                    "contributions": "",
                    "related_work_summary": "",
                    "experimental_setup": "",
                    "reproducibility_info": "",
                    "discussion": "",
                    "conclusion": ""
                },
                "has_full_text": False,
                "github_repos": []
            })
    
    state["lit_matrix"] = lit_matrix
    state["next_step"] = "find_gaps"
    
    papers_with_pdf = sum(1 for p in lit_matrix if p.get("has_full_text", False))
    print(f"   ‚úì –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(lit_matrix)} —Å—Ç–∞—Ç–µ–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
    if papers_with_pdf > 0:
        print(f"   üìÑ –ò–∑ –Ω–∏—Ö —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º PDF: {papers_with_pdf}")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–ò: –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è GapMiner
# ============================================================================

def analyze_temporal_evolution(lit_matrix: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –ê–Ω–∞–ª–∏–∑ —ç–≤–æ–ª—é—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
    –í—ã—è–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã, —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –ø–æ–¥—Ö–æ–¥—ã, –º–µ—Ç–æ–¥—ã –±–µ–∑ —Ä–∞–∑–≤–∏—Ç–∏—è
    """
    gaps = []
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã –ø–æ –≥–æ–¥–∞–º
    methods_by_year = {}
    for paper in lit_matrix:
        year = paper.get("published", "Unknown")
        try:
            year_int = int(year) if year != "Unknown" else None
        except:
            year_int = None
        
        if year_int is None:
            continue
            
        summary = paper.get("structured_summary", {})
        methods = summary.get("methods", [])
        
        if year_int not in methods_by_year:
            methods_by_year[year_int] = []
        methods_by_year[year_int].extend(methods)
    
    if len(methods_by_year) < 2:
        return gaps
    
    # –ù–∞—Ö–æ–¥–∏–º –º–µ—Ç–æ–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –Ω–µ–¥–∞–≤–Ω–æ (–Ω–æ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã)
    recent_years = sorted(methods_by_year.keys())[-2:]
    old_years = sorted(methods_by_year.keys())[:-2] if len(methods_by_year) > 2 else []
    
    recent_methods = set()
    for year in recent_years:
        recent_methods.update(methods_by_year[year])
    
    old_methods = set()
    for year in old_years:
        old_methods.update(methods_by_year[year])
    
    # –ú–µ—Ç–æ–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã —Ä–∞–Ω—å—à–µ, –Ω–æ –∏—Å—á–µ–∑–ª–∏
    disappeared_methods = old_methods - recent_methods
    if disappeared_methods:
        gaps.append({
            "gap": f"–ú–µ—Ç–æ–¥—ã {', '.join(list(disappeared_methods)[:3])} –±—ã–ª–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã —Ä–∞–Ω–µ–µ, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö. –í–æ–∑–º–æ–∂–Ω–æ —É—Å—Ç–∞—Ä–µ–ª–∏ –∏–ª–∏ –∑–∞–º–µ–Ω–µ–Ω—ã.",
            "type": "temporal",
            "severity": "medium",
            "evidence": [p.get("title", "") for p in lit_matrix if any(m in old_methods for m in p.get("structured_summary", {}).get("methods", []))][:3],
            "reasoning": "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–¥–≤–∏–≥ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–æ–≤",
            "potential_impact": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç–≤–æ–ª—é—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–≤ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –±—É–¥—É—â–∏–µ —Ç—Ä–µ–Ω–¥—ã",
            "related_methods": list(recent_methods)[:5],
            "feasibility": "medium"
        })
    
    return gaps


def detect_contradictions(lit_matrix: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö/–º–µ—Ç—Ä–∏–∫–∞—Ö
    """
    gaps = []
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    papers_by_dataset = {}
    for paper in lit_matrix:
        summary = paper.get("structured_summary", {})
        datasets = summary.get("datasets", [])
        for dataset in datasets:
            if dataset not in papers_by_dataset:
                papers_by_dataset[dataset] = []
            papers_by_dataset[dataset].append(paper)
    
    # –ò—â–µ–º –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
    for dataset, papers in papers_by_dataset.items():
        if len(papers) < 2:
            continue
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        findings = []
        for paper in papers:
            summary = paper.get("structured_summary", {})
            findings.append({
                "title": paper.get("title", ""),
                "findings": summary.get("key_findings", ""),
                "methods": summary.get("methods", [])
            })
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –≤—ã–≤–æ–¥—ã (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        if len(set(f["findings"][:100] for f in findings)) > 1:
            gaps.append({
                "gap": f"–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ {dataset}. –†–∞–∑–Ω—ã–µ —Ä–∞–±–æ—Ç—ã –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –≤—ã–≤–æ–¥—ã.",
                "type": "contradiction",
                "severity": "high",
                "evidence": [f["title"] for f in findings],
                "reasoning": "–ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–±–æ—Ç –Ω–∞ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
                "potential_impact": "–í—ã—Å–æ–∫–æ–µ - —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π",
                "related_methods": [m for f in findings for m in f["methods"]],
                "feasibility": "high"
            })
    
    return gaps


def find_methodological_gaps(lit_matrix: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –ê–Ω–∞–ª–∏–∑ –Ω–µ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –º–µ—Ç–æ–¥√ó–¥–∞—Ç–∞—Å–µ—Ç√ó–º–µ—Ç—Ä–∏–∫–∞
    """
    gaps = []
    
    all_methods = set()
    all_datasets = set()
    all_metrics = set()
    
    for paper in lit_matrix:
        summary = paper.get("structured_summary", {})
        all_methods.update(summary.get("methods", []))
        all_datasets.update(summary.get("datasets", []))
        all_metrics.update(summary.get("metrics", []))
        
    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–≥–∏—á–Ω—ã –Ω–æ –Ω–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω—ã
    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –º–µ—Ç–æ–¥ A —É—Å–ø–µ—à–µ–Ω –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ X, –Ω–æ –Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª—Å—è –Ω–∞ –ø–æ—Ö–æ–∂–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ Y
    if len(all_methods) > 0 and len(all_datasets) > 1:
        gaps.append({
            "gap": f"–ú–µ—Ç–æ–¥—ã {', '.join(list(all_methods)[:3])} –Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏.",
            "type": "methodological",
            "severity": "medium",
            "evidence": [p.get("title", "") for p in lit_matrix[:5]],
            "reasoning": "–ê–Ω–∞–ª–∏–∑ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –º–µ—Ç–æ–¥–æ–≤ –∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–±–µ–ª—ã",
            "potential_impact": "–°—Ä–µ–¥–Ω–µ–µ - –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤",
            "related_methods": list(all_methods)[:5],
            "feasibility": "high"
        })
    
    return gaps


def analyze_reproducibility(lit_matrix: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –ê–Ω–∞–ª–∏–∑ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–¥–∞/–¥–∞–Ω–Ω—ã—Ö
    """
    gaps = []
    
    papers_without_code = []
    for paper in lit_matrix:
        summary = paper.get("structured_summary", {})
        github_repos = paper.get("github_repos", [])
        reproducibility_info = summary.get("reproducibility_info", "").lower()
        
        has_code = len(github_repos) > 0 or "github" in reproducibility_info or "code" in reproducibility_info
        
        if not has_code:
            papers_without_code.append(paper.get("title", ""))
    
    if len(papers_without_code) > len(lit_matrix) * 0.5:  # –ë–æ–ª—å—à–µ 50% –±–µ–∑ –∫–æ–¥–∞
        gaps.append({
            "gap": f"–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä–∞–±–æ—Ç ({len(papers_without_code)}/{len(lit_matrix)}) –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∫–æ–¥ –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.",
            "type": "reproducibility",
            "severity": "high",
            "evidence": papers_without_code[:5],
            "reasoning": "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–æ–¥–∞/–¥–∞–Ω–Ω—ã—Ö –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
            "potential_impact": "–í—ã—Å–æ–∫–æ–µ - –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –Ω–∞—É—á–Ω–æ–π –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏",
            "related_methods": [],
            "feasibility": "medium"
        })
    
    return gaps


def analyze_scalability(lit_matrix: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –ê–Ω–∞–ª–∏–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª—è—Ö
    """
    gaps = []
    
    scalability_mentions = []
    for paper in lit_matrix:
        summary = paper.get("structured_summary", {})
        experimental_setup = summary.get("experimental_setup", "").lower()
        discussion = summary.get("discussion", "").lower()
        
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞
        text = experimental_setup + " " + discussion
        if any(keyword in text for keyword in ["large scale", "scalability", "computational cost", "efficiency"]):
            scalability_mentions.append(paper.get("title", ""))
    
    if len(scalability_mentions) < len(lit_matrix) * 0.3:  # –ú–µ–Ω—å—à–µ 30% –æ–±—Å—É–∂–¥–∞—é—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
        gaps.append({
            "gap": "–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä–∞–±–æ—Ç –Ω–µ –æ–±—Å—É–∂–¥–∞—é—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏–ª–∏ computational cost –º–µ—Ç–æ–¥–æ–≤. –ù–µ—è—Å–Ω–æ –∫–∞–∫ –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª—è—Ö.",
            "type": "scalability",
            "severity": "medium",
            "evidence": [p.get("title", "") for p in lit_matrix if p.get("title") not in scalability_mentions][:5],
            "reasoning": "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ",
            "potential_impact": "–°—Ä–µ–¥–Ω–µ–µ - –≤–∞–∂–Ω–æ –¥–ª—è production deployment",
            "related_methods": [],
            "feasibility": "medium"
        })
    
    return gaps


def find_cross_domain_opportunities(lit_matrix: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–µ—Ä–µ–Ω–æ—Å–∞ –º–µ—Ç–æ–¥–æ–≤ –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏
    """
    gaps = []
    
    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –º–µ—Ç–æ–¥—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞ –Ω–µ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –≤ –¥—Ä—É–≥–æ–º
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–º–µ–Ω–æ–≤
    
    all_methods = []
    for paper in lit_matrix:
        summary = paper.get("structured_summary", {})
        methods = summary.get("methods", [])
        all_methods.extend(methods)
    
    if len(set(all_methods)) > 5:
        gaps.append({
            "gap": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ç–æ–¥–æ–≤, –Ω–æ –Ω–µ—è—Å–Ω–æ –∫–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –º–æ–∂–Ω–æ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ –¥—Ä—É–≥–∏–µ –¥–æ–º–µ–Ω—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è –º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.",
            "type": "cross_domain",
            "severity": "low",
            "evidence": [p.get("title", "") for p in lit_matrix[:5]],
            "reasoning": "–ú–µ—Ç–æ–¥—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–∏–º—ã –≤ –¥—Ä—É–≥–∏—Ö –¥–æ–º–µ–Ω–∞—Ö, –Ω–æ —ç—Ç–æ –Ω–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–æ",
            "potential_impact": "–°—Ä–µ–¥–Ω–µ–µ - –º–æ–∂–µ—Ç –æ—Ç–∫—Ä—ã—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤",
            "related_methods": list(set(all_methods))[:5],
            "feasibility": "medium"
        })
    
    return gaps


def llm_deep_gap_analysis(lit_matrix: List[Dict[str, Any]], query: str, llm) -> List[Dict[str, Any]]:
    """
    –ì–ª—É–±–æ–∫–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ gaps —á–µ—Ä–µ–∑ LLM
    """
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        papers_summary = []
        for paper in lit_matrix[:10]:  # –¢–æ–ø-10 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            summary = paper.get("structured_summary", {})
            papers_summary.append({
                "title": paper.get("title", ""),
                "methods": summary.get("methods", []),
                "datasets": summary.get("datasets", []),
                "limitations": summary.get("limitations", ""),
                "future_work": summary.get("future_work", "")
            })
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ—Ä–ø—É—Å –∏–∑ {len(lit_matrix)} –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ "{query}" –∏ –Ω–∞–π–¥–∏ –≥–ª—É–±–æ–∫–∏–µ research gaps.

–°—Ç–∞—Ç—å–∏:
{chr(10).join([f"- {p['title']}: –º–µ—Ç–æ–¥—ã={p['methods']}, limitations={p['limitations'][:100]}" for p in papers_summary])}

–ù–∞–π–¥–∏ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã —á–µ—Ä–µ–∑ reasoning:
1. –õ–æ–≥–∏—á–Ω—ã–µ –Ω–æ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–≤
2. –ú–µ—Ç–æ–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–æ –Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å
3. –ü—Ä–æ–±–ª–µ–º—ã –∫–æ—Ç–æ—Ä—ã–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –Ω–æ –Ω–µ —Ä–µ—à–∞—é—Ç—Å—è
4. –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –≤—ã–≤–æ–¥–∞—Ö –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è

–í–µ—Ä–Ω–∏ JSON:
{{
  "gaps": [
    {{
      "gap": "–æ–ø–∏—Å–∞–Ω–∏–µ –ª–∞–∫—É–Ω—ã",
      "type": "methodological|data|metric|reproducibility|contradiction|temporal|scalability|cross_domain",
      "severity": "high|medium|low",
      "evidence": ["–Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π"],
      "reasoning": "–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø–æ—á–µ–º—É —ç—Ç–æ gap",
      "potential_impact": "–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ",
      "related_methods": ["–º–µ—Ç–æ–¥—ã"],
      "feasibility": "high|medium|low"
    }}
  ]
}}
"""
    
        messages = [
            SystemMessage(content="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –ø–æ–∏—Å–∫—É –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã—Ö research gaps."),
            HumanMessage(content=prompt)
        ]
        
        response = llm.invoke(messages)
        parsed = safe_json_parse(response.content, {})
        
        gaps = []
        if isinstance(parsed, dict):
            for gap_dict in parsed.get("gaps", []):
                try:
                    gap_model = ResearchGapModel(**gap_dict)
                    gaps.append(gap_model.model_dump())
                except:
                    gaps.append({
                        "gap": gap_dict.get("gap", ""),
                        "type": gap_dict.get("type", "general"),
                        "severity": gap_dict.get("severity", "medium"),
                        "evidence": gap_dict.get("evidence", []),
                        "reasoning": gap_dict.get("reasoning", ""),
                        "potential_impact": gap_dict.get("potential_impact", ""),
                        "related_methods": gap_dict.get("related_methods", []),
                        "feasibility": gap_dict.get("feasibility", "medium")
                    })
        
        return gaps
    except Exception as e:
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 13: GapMiner - –ø–æ–∏—Å–∫ research gaps
# ============================================================================

def find_research_gaps(state: AgentState) -> AgentState:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –∏ –≤—ã—è–≤–ª—è–µ—Ç research gaps –∏—Å–ø–æ–ª—å–∑—É—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    """
    print("\nüî¨ –§—É–Ω–∫—Ü–∏—è 13: GapMiner - –ø–æ–∏—Å–∫ research gaps...")
    
    lit_matrix = state["lit_matrix"]
    llm = get_llm()
    query = state.get("query", "")
    
    gap_list = []
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
    print("   üìä –ü—Ä–∏–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–Ω–∞–ª–∏–∑–∞...")
    
    # 1. Temporal Analysis
    temporal_gaps = analyze_temporal_evolution(lit_matrix)
    gap_list.extend(temporal_gaps)
    if temporal_gaps:
        print(f"      ‚úì Temporal analysis: –Ω–∞–π–¥–µ–Ω–æ {len(temporal_gaps)} gaps")
    
    # 2. Contradiction Detection
    contradiction_gaps = detect_contradictions(lit_matrix)
    gap_list.extend(contradiction_gaps)
    if contradiction_gaps:
        print(f"      ‚úì Contradiction detection: –Ω–∞–π–¥–µ–Ω–æ {len(contradiction_gaps)} gaps")
    
    # 3. Methodological Gaps
    methodological_gaps = find_methodological_gaps(lit_matrix)
    gap_list.extend(methodological_gaps)
    if methodological_gaps:
        print(f"      ‚úì Methodological analysis: –Ω–∞–π–¥–µ–Ω–æ {len(methodological_gaps)} gaps")
    
    # 4. Reproducibility Analysis
    repro_gaps = analyze_reproducibility(lit_matrix)
    gap_list.extend(repro_gaps)
    if repro_gaps:
        print(f"      ‚úì Reproducibility analysis: –Ω–∞–π–¥–µ–Ω–æ {len(repro_gaps)} gaps")
    
    # 5. Scalability Analysis
    scale_gaps = analyze_scalability(lit_matrix)
    gap_list.extend(scale_gaps)
    if scale_gaps:
        print(f"      ‚úì Scalability analysis: –Ω–∞–π–¥–µ–Ω–æ {len(scale_gaps)} gaps")
    
    # 6. Cross-domain Opportunities
    cross_domain_gaps = find_cross_domain_opportunities(lit_matrix)
    gap_list.extend(cross_domain_gaps)
    if cross_domain_gaps:
        print(f"      ‚úì Cross-domain analysis: –Ω–∞–π–¥–µ–Ω–æ {len(cross_domain_gaps)} gaps")
    
    # 7. LLM-based Deep Analysis
    try:
        llm_gaps = llm_deep_gap_analysis(lit_matrix, query, llm)
        gap_list.extend(llm_gaps)
        if llm_gaps:
            print(f"      ‚úì LLM deep analysis: –Ω–∞–π–¥–µ–Ω–æ {len(llm_gaps)} gaps")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM deep analysis: {e}")
    
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –≤—Å–µ gaps —á–µ—Ä–µ–∑ Pydantic
    validated_gaps = []
    for gap_dict in gap_list:
        try:
            gap_model = ResearchGapModel(**gap_dict)
            validated_gaps.append(gap_model.model_dump())
        except ValidationError:
            # Fallback —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π
            validated_gaps.append({
                "gap": gap_dict.get("gap", ""),
                "type": gap_dict.get("type", "general"),
                "severity": gap_dict.get("severity", "medium"),
                "evidence": gap_dict.get("evidence", []),
                "reasoning": gap_dict.get("reasoning", ""),
                "potential_impact": gap_dict.get("potential_impact", ""),
                "related_methods": gap_dict.get("related_methods", []),
                "feasibility": gap_dict.get("feasibility", "medium")
            })
    
    gap_list = validated_gaps
    
    state["gap_list"] = gap_list
    
    print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ research gaps: {len(gap_list)}")
    for i, gap in enumerate(gap_list[:3], 1):
        print(f"      {i}. [{gap.get('severity', 'N/A')}] {gap.get('gap', 'N/A')[:80]}")
    
    # –ê–≥–µ–Ω—Ç–Ω–æ–µ –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
    try:
        from config import EXPERIMENTAL, REPLAN_CONFIG
        if EXPERIMENTAL.get("enable_replanning", False) and REPLAN_CONFIG.get("enabled", False):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–º
            coverage_issues = analyze_coverage(lit_matrix, gap_list)
            if coverage_issues:
                print(f"   üîÑ –ü–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑-–∑–∞ –ø—Ä–æ–≤–∞–ª–æ–≤ –ø–æ–∫—Ä—ã—Ç–∏—è...")
                new_queries = replan_queries(state["query"], coverage_issues, llm)
                if new_queries:
                    state["query_strings"].extend(new_queries)
                    print(f"      ‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_queries)} –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    except Exception as e:
        pass
    
    state["next_step"] = "generate_ideas"
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–ò: –ê–≥–µ–Ω—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ - –Ω–æ–≤—ã–µ —É–∑–ª—ã
# ============================================================================

def retry_search_with_expansion(state: AgentState) -> AgentState:
    """
    –†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–∏—Å–∫ –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    print("\nüîÑ Retry Search - —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
    
    llm = get_llm()
    query = state["query"]
    retry_count = state.get("retry_count", 0)
    
    try:
        from config import AGENT_CONFIG
        max_retries = AGENT_CONFIG.get("max_retries", 2)
    except:
        max_retries = 2
    
    if retry_count >= max_retries:
        print("   ‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–∫—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
        state["next_step"] = "rank"
        return state
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –ø–æ–º–æ—â—å—é LLM
    expansion_prompt = f"""–¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –¥–∞–ª –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –†–∞—Å—à–∏—Ä—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.

–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {query}
–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(state.get('corpus_index', []))}

–°–æ–∑–¥–∞–π 3 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞ –∫–æ—Ç–æ—Ä—ã–µ:
1. –ò—Å–ø–æ–ª—å–∑—É—é—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–º–µ–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
2. –†–∞—Å—à–∏—Ä—è—é—Ç –æ–±–ª–∞—Å—Ç—å –ø–æ–∏—Å–∫–∞
3. –í–∫–ª—é—á–∞—é—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

–í–µ—Ä–Ω–∏ JSON:
{{
  "expanded_queries": ["–∑–∞–ø—Ä–æ—Å 1", "–∑–∞–ø—Ä–æ—Å 2", "–∑–∞–ø—Ä–æ—Å 3"]
}}
"""
    
    try:
        messages = [
            SystemMessage(content="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞—É—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."),
            HumanMessage(content=expansion_prompt)
        ]
        response = llm.invoke(messages)
        parsed = safe_json_parse(response.content, {})
        expanded_queries = parsed.get("expanded_queries", [query])
        
        # –û–±–Ω–æ–≤–ª—è–µ–º query_strings
        state["query_strings"] = expanded_queries
        state["retry_count"] = retry_count + 1
        
        print(f"   ‚úì –°–æ–∑–¥–∞–Ω–æ {len(expanded_queries)} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"   üîç –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        state["next_step"] = "retrieve"
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {e}")
        state["next_step"] = "rank"
    
    return state


def replan_search_queries(state: AgentState) -> AgentState:
    """
    –ü–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö gaps
    """
    print("\nüîÑ Replan Search - –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ gaps...")
    
    llm = get_llm()
    gap_list = state.get("gap_list", [])
    query = state["query"]
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º gaps –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    high_gaps = [g for g in gap_list if g.get("severity") == "high"]
    
    if not high_gaps:
        print("   ‚ÑπÔ∏è  –ù–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö gaps –¥–ª—è –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        state["next_step"] = "generate_ideas"
        return state
    
    replan_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö research gaps —Å–æ–∑–¥–∞–π –Ω–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã.

–ò—Å—Ö–æ–¥–Ω–∞—è —Ç–µ–º–∞: {query}

–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ gaps:
{chr(10).join([f"- {g.get('gap', '')[:200]}" for g in high_gaps[:5]])}

–°–æ–∑–¥–∞–π 3-5 –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤.

–í–µ—Ä–Ω–∏ JSON:
{{
  "new_queries": ["–∑–∞–ø—Ä–æ—Å 1", "–∑–∞–ø—Ä–æ—Å 2", "–∑–∞–ø—Ä–æ—Å 3"]
}}
"""
    
    try:
        messages = [
            SystemMessage(content="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞—É—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è research gaps."),
            HumanMessage(content=replan_prompt)
        ]
        response = llm.invoke(messages)
        parsed = safe_json_parse(response.content, {})
        new_queries = parsed.get("new_queries", [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        existing_queries = state.get("query_strings", [query])
        state["query_strings"] = existing_queries + new_queries
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–π
        replanning_history = state.get("replanning_history", [])
        replanning_history.append(f"Gaps-based replan: {len(new_queries)} queries")
        state["replanning_history"] = replanning_history
        
        print(f"   ‚úì –°–æ–∑–¥–∞–Ω–æ {len(new_queries)} –Ω–æ–≤—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        state["next_step"] = "retrieve"
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    state["next_step"] = "generate_ideas"
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–ò: –ê–≥–µ–Ω—Ç–Ω–æ–µ –ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
# ============================================================================

def analyze_coverage(lit_matrix: List[Dict[str, Any]], gap_list: List[Dict[str, Any]]) -> List[str]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–º –∏ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–≤–∞–ª—ã
    """
    issues = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ high-severity gaps
    high_gaps = [g for g in gap_list if g.get("severity") == "high"]
    if len(high_gaps) > 3:
        issues.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(high_gaps)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö research gaps")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ç–æ–¥–æ–≤
    all_methods = set()
    for paper in lit_matrix:
        summary = paper.get("structured_summary", {})
        all_methods.update(summary.get("methods", []))
    
    if len(all_methods) < 5:
        issues.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ç–æ–¥–æ–≤ –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö")
    
    return issues


def replan_queries(original_query: str, coverage_issues: List[str], llm) -> List[str]:
    """
    –ü–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–∞–ª–æ–≤ –ø–æ–∫—Ä—ã—Ç–∏—è
    """
    if not coverage_issues:
        return []
    
    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞—É—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.

–ù–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º, —Å–æ–∑–¥–∞–π –Ω–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞.

–í–µ—Ä–Ω–∏ JSON:
{
  "new_queries": ["–Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 1", "–Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 2", ...]
}
"""
    
    user_prompt = f"""–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {original_query}

–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
{chr(10).join([f"- {issue}" for issue in coverage_issues])}

–°–æ–∑–¥–∞–π 2-3 –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º."""
    
    try:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = llm.invoke(messages)
        parsed = safe_json_parse(response.content, {})
        
        if isinstance(parsed, dict):
            return parsed.get("new_queries", [])
        return []
    
    except Exception as e:
        return []


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 14: Ideator - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–¥–µ–π
# ============================================================================

def generate_research_ideas(state: AgentState) -> AgentState:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö gaps
    """
    print("\nüí° –§—É–Ω–∫—Ü–∏—è 14: Ideator - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–¥–µ–π...")
    
    gap_list = state["gap_list"]
    lit_matrix = state["lit_matrix"]
    llm = get_llm()
    
    ideation_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö research gaps –ø—Ä–µ–¥–ª–æ–∂–∏ 5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö, –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –∏–¥–µ–π –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.

**Research Gaps:**
{chr(10).join([f"- {g.get('gap', 'N/A')}" for g in gap_list[:5]])}

**–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ç–æ–ø —Ä–∞–±–æ—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏):**
{chr(10).join([f"- {p.get('title', 'Unknown')[:60]}" for p in lit_matrix[:3]])}

–î–ª—è –∫–∞–∂–¥–æ–π –∏–¥–µ–∏ —É–∫–∞–∂–∏:
1. –ì–∏–ø–æ—Ç–µ–∑—É (—á—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º)
2. –ü–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–º–µ—Ç–æ–¥—ã, –¥–∞—Ç–∞—Å–µ—Ç—ã, –±–µ–π–∑–ª–∞–π–Ω—ã)
3. –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
4. –†–∏—Å–∫–∏ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã

–í–µ—Ä–Ω–∏ JSON:
{{
  "ideas": [
    {{
      "hypothesis": "–æ–ø–∏—Å–∞–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã",
      "experiment_plan": {{
        "methods": ["–º–µ—Ç–æ–¥ 1"],
        "datasets": ["–¥–∞—Ç–∞—Å–µ—Ç 1"],
        "baselines": ["–±–µ–π–∑–ª–∞–π–Ω 1"],
        "metrics": ["–º–µ—Ç—Ä–∏–∫–∞ 1"]
      }},
      "expected_outcome": "—á—Ç–æ –æ–∂–∏–¥–∞–µ–º",
      "risks": ["—Ä–∏—Å–∫ 1"],
      "related_gap": "–Ω–∞ –∫–∞–∫–æ–π gap –æ—Ç–≤–µ—á–∞–µ—Ç"
    }}
  ]
}}
"""
    
    try:
        messages = [
            SystemMessage(content="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã."),
            HumanMessage(content=ideation_prompt)
        ]
        
        response = llm.invoke(messages)
        
        # –ü–∞—Ä—Å–∏–º —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        parsed = safe_json_parse(response.content, {})
        ideas_data = parsed if isinstance(parsed, dict) else {}
        
        ideas_raw = ideas_data.get("ideas", [])
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ Pydantic
        idea_bank = []
        for idea_dict in ideas_raw:
            try:
                idea_model = ResearchIdeaModel(**idea_dict)
                idea_bank.append(idea_model.model_dump())
            except ValidationError:
                # Fallback
                idea_bank.append({
                    "hypothesis": idea_dict.get("hypothesis", ""),
                    "experiment_plan": idea_dict.get("experiment_plan", {}),
                    "expected_outcome": idea_dict.get("expected_outcome", ""),
                    "risks": idea_dict.get("risks", []),
                    "related_gap": idea_dict.get("related_gap", "")
                })
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–¥–µ–π: {e}")
        idea_bank = [
            {
                "hypothesis": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–¥–µ–π",
                "experiment_plan": {},
                "expected_outcome": "N/A",
                "risks": [],
                "related_gap": "general"
            }
        ]
    
    state["idea_bank"] = idea_bank
    state["next_step"] = "report"
    
    print(f"   ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏–¥–µ–π: {len(idea_bank)}")
    for i, idea in enumerate(idea_bank[:3], 1):
        print(f"      {i}. {idea.get('hypothesis', 'N/A')[:70]}...")
    
    return state


# ============================================================================
# –§–£–ù–ö–¶–ò–Ø 15: Reporter - —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
# ============================================================================

def generate_final_report(state: AgentState) -> AgentState:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    """
    print("\nüìÑ –§—É–Ω–∫—Ü–∏—è 16: Reporter - —Å–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞...")
    
    query = state["query"]
    lit_matrix = state.get("lit_matrix", [])
    gap_list = state.get("gap_list", [])
    idea_bank = state.get("idea_bank", [])
    ranked_papers = state.get("ranked_papers", [])
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
    report = f"""# –û—Ç—á—ë—Ç –ø–æ –Ω–∞—É—á–Ω—ã–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º

## –¢–µ–º–∞: {query}

**–î–∞—Ç–∞:** 2025-10-20  
**–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π:** {len(ranked_papers)}  
**–î–µ—Ç–∞–ª—å–Ω–æ –∏–∑—É—á–µ–Ω–æ:** {len(lit_matrix)}

---

## 1. Executive Summary

–ü—Ä–æ–≤–µ–¥—ë–Ω —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ "{query}".
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (OpenAlex, Semantic Scholar, Crossref, ArXiv, PubMed).
–í—ã—è–≤–ª–µ–Ω–æ {len(gap_list)} research gaps –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ {len(idea_bank)} –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–¥–µ–π.

---

## 2. –¢–æ–ø-—Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

"""
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-10 —Å—Ç–∞—Ç–µ–π
    for i, paper in enumerate(ranked_papers[:10], 1):
        authors_str = ", ".join(paper.get("authors", [])[:3])
        if len(paper.get("authors", [])) > 3:
            authors_str += " et al."
        
        report += f"""### {i}. {paper.get('title', 'No title')}

- **–ê–≤—Ç–æ—Ä—ã:** {authors_str}
- **–ì–æ–¥:** {paper.get('published')}
- **Venue:** {paper.get('venue', 'Unknown')}
- **–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π:** {paper.get('citations_total', 0)}
- **Score:** {paper.get('relevance_score', 0):.3f}
- **URL:** {paper.get('url', 'N/A')}

"""
        if paper.get("doi"):
            report += f"- **DOI:** [{paper['doi']}](https://doi.org/{paper['doi']})\n"
        
        report += f"\n**–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è:** {paper.get('summary', 'No abstract')[:300]}...\n\n---\n\n"
    
    # –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
    if lit_matrix:
        report += f"""## 3. –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–ø-{len(lit_matrix)})

| –†–∞–±–æ—Ç–∞ | –ü—Ä–æ–±–ª–µ–º–∞ | –ú–µ—Ç–æ–¥—ã | –î–∞—Ç–∞—Å–µ—Ç—ã | –ú–µ—Ç—Ä–∏–∫–∏ | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è | PDF |
|--------|----------|--------|----------|---------|-------------|-----|
"""
        
        for paper in lit_matrix:
            summary = paper.get("structured_summary", {})
            # ============================================================================
            # –£–õ–£–ß–®–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–µ–Ω—ã –ª–∏–º–∏—Ç—ã –æ–±—Ä–µ–∑–∞–Ω–∏—è (–ó–∞–¥–∞—á–∞ 1.3)
            # ============================================================================
            title_short = paper.get('title', 'N/A')[:60]  # –±—ã–ª–æ 40
            problem = summary.get('problem', 'N/A')[:100]  # –±—ã–ª–æ 50
            methods = ", ".join(summary.get('methods', [])[:3])[:80]  # –±—ã–ª–æ [:2][:30]
            datasets = ", ".join(summary.get('datasets', [])[:3])[:60]  # –±—ã–ª–æ [:2][:30]
            metrics = ", ".join(summary.get('metrics', [])[:3])[:60]  # –±—ã–ª–æ [:2][:30]
            limitations = summary.get('limitations', 'N/A')[:80]  # –±—ã–ª–æ 40
            has_pdf = "üìÑ" if paper.get('has_full_text', False) else "‚Äî"
            
            report += f"| {title_short}... | {problem} | {methods} | {datasets} | {metrics} | {limitations} | {has_pdf} |\n"
        
        report += "\n---\n\n"
        
        # ============================================================================
        # –ù–û–í–´–ô –†–ê–ó–î–ï–õ: Integrated Synthesis (–ó–∞–¥–∞—á–∞ 1.1)
        # ============================================================================
        report += """## 3.5. üìä –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç:

"""
        
        for idx, paper in enumerate(lit_matrix, 1):
            title = paper.get('title', 'Unknown')
            summary = paper.get("structured_summary", {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–µ (–Ω–µ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–µ) –¥–∞–Ω–Ω—ã–µ
            findings = summary.get('key_findings', '')
            methods = summary.get('methods', [])
            contributions = summary.get('contributions', '')
            limitations = summary.get('limitations', '')
            
            report += f"**üìÑ [{idx}] {title}**\n\n"
            
            # –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã (–ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç, –Ω–µ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π)
            if findings and findings != "N/A" and findings != "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å":
                report += f"*–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:* {findings}\n\n"
            
            # –ú–µ—Ç–æ–¥—ã (–¥–æ 5 –º–µ—Ç–æ–¥–æ–≤)
            if methods and len(methods) > 0:
                methods_str = ", ".join(methods[:5])
                report += f"*–ú–µ—Ç–æ–¥—ã:* {methods_str}\n\n"
            
            # –í–∫–ª–∞–¥ —Ä–∞–±–æ—Ç—ã
            if contributions and contributions != "N/A" and contributions != "":
                report += f"*–í–∫–ª–∞–¥:* {contributions}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            if limitations and limitations != "N/A" and limitations != "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏":
                report += f"*–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:* {limitations}\n\n"
            
            report += "---\n\n"
        
        # –û–±—â–∏–π —Å–∏–Ω—Ç–µ–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        report += "### üîç –û–±—â–∏–µ —Ç—Ä–µ–Ω–¥—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:\n\n"
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
        all_methods = []
        all_datasets = []
        for paper in lit_matrix:
            summary = paper.get("structured_summary", {})
            all_methods.extend(summary.get('methods', []))
            all_datasets.extend(summary.get('datasets', []))
        
        if all_methods:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –º–µ—Ç–æ–¥–æ–≤
            from collections import Counter
            method_counts = Counter(all_methods)
            top_methods = method_counts.most_common(5)
            
            report += "**–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**\n"
            for method, count in top_methods:
                report += f"- {method} (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ {count} —Ä–∞–±–æ—Ç–∞—Ö)\n"
            report += "\n"
        
        if all_datasets:
            dataset_counts = Counter(all_datasets)
            top_datasets = dataset_counts.most_common(5)
            
            report += "**–ù–∞–∏–±–æ–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã:**\n"
            for dataset, count in top_datasets:
                report += f"- {dataset} (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ {count} —Ä–∞–±–æ—Ç–∞—Ö)\n"
            report += "\n"
        
        report += "\n---\n\n"
    
    # Research Gaps
    if gap_list:
        report += f"""## 4. Research Gaps (–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ª–∞–∫—É–Ω—ã)

–í—ã—è–≤–ª–µ–Ω–æ **{len(gap_list)}** –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:

"""
        for i, gap in enumerate(gap_list, 1):
            severity_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(gap.get('severity', 'medium'), "‚ö™")
            report += f"""### Gap {i}: {severity_emoji} {gap.get('severity', 'N/A').upper()}

**–¢–∏–ø:** {gap.get('type', 'N/A')}

**–û–ø–∏—Å–∞–Ω–∏–µ:** {gap.get('gap', 'N/A')}

**–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:**
"""
            for evidence in gap.get('evidence', [])[:3]:
                report += f"- {evidence}\n"
            
            report += "\n---\n\n"
    
    # Research Ideas
    if idea_bank:
        report += f"""## 5. –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–¥–µ–∏

–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ **{len(idea_bank)}** –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –≥–∏–ø–æ—Ç–µ–∑:

"""
        for i, idea in enumerate(idea_bank, 1):
            report += f"""### –ò–¥–µ—è {i}

**–ì–∏–ø–æ—Ç–µ–∑–∞:** {idea.get('hypothesis', 'N/A')}

**–ü–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:**
"""
            plan = idea.get('experiment_plan', {})
            if plan.get('methods'):
                report += f"- **–ú–µ—Ç–æ–¥—ã:** {', '.join(plan['methods'])}\n"
            if plan.get('datasets'):
                report += f"- **–î–∞—Ç–∞—Å–µ—Ç—ã:** {', '.join(plan['datasets'])}\n"
            if plan.get('baselines'):
                report += f"- **–ë–µ–π–∑–ª–∞–π–Ω—ã:** {', '.join(plan['baselines'])}\n"
            if plan.get('metrics'):
                report += f"- **–ú–µ—Ç—Ä–∏–∫–∏:** {', '.join(plan['metrics'])}\n"
            
            report += f"\n**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** {idea.get('expected_outcome', 'N/A')}\n\n"
            
            if idea.get('risks'):
                report += f"**–†–∏—Å–∫–∏:**\n"
                for risk in idea['risks']:
                    report += f"- {risk}\n"
            
            report += f"\n**–°–≤—è–∑–∞–Ω–Ω—ã–π Gap:** {idea.get('related_gap', 'N/A')}\n\n---\n\n"
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    pdf_texts = state.get('pdf_texts', {})
    papers_with_pdf = len(pdf_texts)
    
    report += f"""## 6. –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

- **–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π (—Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏):** {len(state.get('seed_results', []))}
- **–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:** {len(state.get('corpus_index', []))}
- **–¢–æ–ø-—Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:** {len(ranked_papers)}
- **–î–µ—Ç–∞–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:** {len(lit_matrix)}
- **PDF –ø—Ä–æ—á–∏—Ç–∞–Ω–æ:** {papers_with_pdf}
- **API –≤—ã–∑–æ–≤–æ–≤:** {state.get('budget', {}).get('api_calls', 0)}

---

## 7. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–î–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ "{query}".
–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ research gaps –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –∏–¥–µ–∏ –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í—Å–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω—ã —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏. 
–î–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ø–æ–ª–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ —Å—Ç–∞—Ç–µ–π.

---

*–û—Ç—á—ë—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω AI –∞–≥–µ–Ω—Ç–æ–º –Ω–∞ –±–∞–∑–µ LangGraph*
"""
    
    state["final_response"] = report
    state["next_step"] = "end"
    
    print(f"   ‚úì –û—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω: {len(report)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   ‚úì –í–∫–ª—é—á–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: 7")
    
    return state


# ============================================================================
# –°–û–ó–î–ê–ù–ò–ï LANGGRAPH –ê–ì–ï–ù–¢–ê
# ============================================================================

def create_research_agent():
    """
    –°–æ–∑–¥–∞—ë—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç LangGraph –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    –° —É—Å–ª–æ–≤–Ω—ã–º–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
    """
    print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ LangGraph –∞–≥–µ–Ω—Ç–∞ —Å –∞–≥–µ–Ω—Ç–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º...")
    
    # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ
    workflow = StateGraph(AgentState)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã (nodes)
    workflow.add_node("build_topic_card", build_topic_card)
    workflow.add_node("select_sources", analyze_query_and_select_sources)  # –ù–æ–≤—ã–π —É–∑–µ–ª (–§–∞–∑–∞ 4)
    workflow.add_node("retrieve", multi_source_retriever)
    workflow.add_node("deduplicate", deduplicate_and_normalize)
    workflow.add_node("retry_search", retry_search_with_expansion)  # –ù–æ–≤—ã–π —É–∑–µ–ª
    workflow.add_node("rank", hybrid_ranker)
    workflow.add_node("read_pdfs", read_pdfs)
    workflow.add_node("summarize", create_literature_matrix)
    workflow.add_node("find_gaps", find_research_gaps)
    workflow.add_node("replan_search", replan_search_queries)  # –ù–æ–≤—ã–π —É–∑–µ–ª
    workflow.add_node("generate_ideas", generate_research_ideas)
    workflow.add_node("report", generate_final_report)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä—ë–±—Ä–∞ (edges) - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    workflow.set_entry_point("build_topic_card")
    workflow.add_edge("build_topic_card", "select_sources")  # –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    workflow.add_edge("select_sources", "retrieve")  # –ó–∞—Ç–µ–º –ø–æ–∏—Å–∫
    workflow.add_edge("retrieve", "deduplicate")
    
    # –£—Å–ª–æ–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –ø–æ—Å–ª–µ deduplicate
    def route_after_dedup(state: AgentState) -> str:
        try:
            from config import AGENT_CONFIG
            min_papers = AGENT_CONFIG.get("min_papers_threshold", 10)
            enable_retry = AGENT_CONFIG.get("enable_retry", True)
        except:
            min_papers = 10
            enable_retry = True
        
        corpus_size = len(state.get("corpus_index", []))
        retry_count = state.get("retry_count", 0)
        
        if enable_retry and corpus_size < min_papers and retry_count < 2:
            return "retry_search"
        return "rank"
    
    workflow.add_conditional_edges(
        "deduplicate",
        route_after_dedup,
        {
            "retry_search": "retry_search",
            "rank": "rank"
        }
    )
    workflow.add_edge("retry_search", "retrieve")  # –¶–∏–∫–ª –æ–±—Ä–∞—Ç–Ω–æ –∫ –ø–æ–∏—Å–∫—É
    
    workflow.add_edge("rank", "read_pdfs")
    workflow.add_edge("read_pdfs", "summarize")
    workflow.add_edge("summarize", "find_gaps")
    
    # –£—Å–ª–æ–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –ø–æ—Å–ª–µ find_gaps
    def route_after_gaps(state: AgentState) -> str:
        try:
            from config import AGENT_CONFIG
            enable_replan = AGENT_CONFIG.get("enable_replanning", True)
            replan_threshold = AGENT_CONFIG.get("replan_gap_threshold", 5)
        except:
            enable_replan = True
            replan_threshold = 5
        
        if not enable_replan:
            return "generate_ideas"
        
        gap_list = state.get("gap_list", [])
        high_gaps = [g for g in gap_list if g.get("severity") == "high"]
        
        if len(high_gaps) > replan_threshold:
            return "replan_search"
        return "generate_ideas"
    
    workflow.add_conditional_edges(
        "find_gaps",
        route_after_gaps,
        {
            "replan_search": "replan_search",
            "generate_ideas": "generate_ideas"
        }
    )
    workflow.add_edge("replan_search", "retrieve")  # –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ø–æ–∏—Å–∫—É
    
    workflow.add_edge("generate_ideas", "report")
    workflow.add_edge("report", END)
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –≥—Ä–∞—Ñ
    app = workflow.compile()
    
    print("   ‚úì LangGraph –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω —Å 11 —É–∑–ª–∞–º–∏ (–≤–∫–ª—é—á–∞—è –∞–≥–µ–Ω—Ç–Ω—ã–µ)")
    print("   ‚úì Pipeline —Å —É—Å–ª–æ–≤–Ω—ã–º–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏:")
    print("      - Retry search –µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("      - Replan search –µ—Å–ª–∏ –º–Ω–æ–≥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö gaps")
    
    return app


# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def run_research_pipeline(
    query: str,
    time_window: int = 5,
    max_papers: int = 40,
    save_report: bool = True
) -> str:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π pipeline –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    
    Args:
        query: –¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        time_window: –û–∫–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ (–ª–µ—Ç –Ω–∞–∑–∞–¥)
        max_papers: –ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        save_report: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –æ—Ç—á—ë—Ç –≤ —Ñ–∞–π–ª
    
    Returns:
        –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (markdown)
    """
    print("="*70)
    print("üöÄ –ó–ê–ü–£–°–ö AI –ê–ì–ï–ù–¢–ê –î–õ–Ø –ü–û–ò–°–ö–ê –ù–ê–£–ß–ù–´–• –°–¢–ê–¢–ï–ô")
    print("="*70)
    print(f"–¢–µ–º–∞: {query}")
    print(f"–ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {time_window} –ª–µ—Ç")
    print(f"–ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π: {max_papers}")
    print("="*70)
    
    # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞
    agent = create_research_agent()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    initial_state = {
        "query": query,
        "time_window": time_window,
        "max_papers": max_papers,
        "selected_databases": [],
        "refined_query": query,
        "topic_card": {},
        "query_strings": [],
        "seed_results": [],
        "search_results": {},
        "corpus_index": [],
        "ranked_papers": [],
        "pdf_texts": {},
        "citation_graph": {},
        "lit_matrix": [],
        "gap_list": [],
        "idea_bank": [],
        "final_response": "",
        "messages": [],
        "next_step": "",
        "budget": {"api_calls": 0, "llm_calls": 0},
        "retry_count": 0,
        "search_quality_score": 0.0,
        "replanning_history": []
    }
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º pipeline
    print("\nüîÑ –ó–∞–ø—É—Å–∫ pipeline...\n")
    
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥—Ä–∞—Ñ
        final_state = agent.invoke(initial_state)
        
        print("\n" + "="*70)
        print("‚úÖ PIPELINE –ó–ê–í–ï–†–®–Å–ù –£–°–ü–ï–®–ù–û!")
        print("="*70)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        report = final_state.get("final_response", "–û—Ç—á—ë—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
        if save_report:
            filename = f"research_report_{query.replace(' ', '_')[:30]}.md"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nüìÅ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")
        
        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   - –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(final_state.get('corpus_index', []))}")
        print(f"   - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–µ—Ç–∞–ª—å–Ω–æ: {len(final_state.get('lit_matrix', []))}")
        print(f"   - –í—ã—è–≤–ª–µ–Ω–æ research gaps: {len(final_state.get('gap_list', []))}")
        print(f"   - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏–¥–µ–π: {len(final_state.get('idea_bank', []))}")
        print(f"   - API –≤—ã–∑–æ–≤–æ–≤: {final_state.get('budget', {}).get('api_calls', 0)}")
        
        return report
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
    """
    print("\n" + "="*70)
    print("ü§ñ AI –ê–ì–ï–ù–¢ –î–õ–Ø –ü–û–ò–°–ö–ê –ù–ê–£–ß–ù–´–• –°–¢–ê–¢–ï–ô")
    print("–í–µ—Ä—Å–∏—è: 1.0 | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: LangGraph | –ò—Å—Ç–æ—á–Ω–∏–∫–∏: 5")
    print("="*70)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
    if not os.getenv("OPENAI_API_KEY"):
        print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –Ω–∞–π–¥–µ–Ω OPENAI_API_KEY")
        print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏ –¥–æ–±–∞–≤—å—Ç–µ: OPENAI_API_KEY=your_key_here")
        print("\n–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º.\n")
    
    # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
    examples = [
        "transformer models for natural language processing",
        "CRISPR gene editing in cancer treatment",
        "quantum computing algorithms",
        "climate change machine learning predictions"
    ]
    
    print("\nüìã –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–º –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:")
    for i, example in enumerate(examples, 1):
        print(f"   {i}. {example}")
    
    print("\n" + "-"*70)
    print("‚ÑπÔ∏è  –î–ª—è –∑–∞–ø—É—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
    print("   report = run_research_pipeline('–≤–∞—à–∞ —Ç–µ–º–∞', time_window=5, max_papers=40)")
    print("-"*70)
    
    # –î–µ–º–æ-–∑–∞–ø—É—Å–∫ (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)
    # if os.getenv("OPENAI_API_KEY"):
    #     print("\nüé¨ –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ —Å —Ç–µ–º–æ–π: transformer models...")
    #     report = run_research_pipeline(
    #         query="transformer models for natural language processing",
    #         time_window=3,
    #         max_papers=20
    #     )
    #     if report:
    #         print("\n‚úÖ –î–µ–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –æ—Ç—á—ë—Ç–∞.")


if __name__ == "__main__":
    main()


print("\n‚úÖ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã! –ê–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python main.py")
print("   –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: run_research_pipeline('–≤–∞—à–∞ —Ç–µ–º–∞')")

